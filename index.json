[{"categories":["论文笔记"],"content":"ShuffleNet是旷视科技最近提出的一种计算高效的CNN模型，包括v1和v2","date":"2021-11-10","objectID":"/2021/11/shufflenet/","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["论文笔记"],"content":"v1 Paper: Zhang, Xiangyu, Xinyu Zhou, Mengxiao Lin, and Jian Sun. “Shufflenet: An extremely efficient convolutional neural network for mobile devices.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6848-6856. 2018. remark：Shuffle group cited by： 3121 v2 Paper: Ma, Ningning, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. “Shufflenet v2: Practical guidelines for efficient cnn architecture design.” In Proceedings of the European conference on computer vision (ECCV), pp. 116-131. 2018. remark：v1上的改进 cited by： 1668 ","date":"2021-11-10","objectID":"/2021/11/shufflenet/:0:0","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["论文笔记"],"content":"1. 概要 Shufflenet和mobilenet是同年提出的工作，两者从两个不同的角度来分析卷积，尽量在降低计算量和参数量的情况下，保持卷积的使用效率。mobilenet通过深度可分离卷积来实现目的，而shufflenet利用了分组卷积来降低参数量，利用shuffle的操作来增强不同通道之间的交互和融合。 ","date":"2021-11-10","objectID":"/2021/11/shufflenet/:0:1","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["论文笔记"],"content":"2. 分组点卷积（pointwise group convolution） 2.1 分组卷积（group convolution） 分组卷积（Group Convolution）最早出现在AlexNet中。 当时使用分组卷积是因为一个GPU难以直接训练整个网络，所以使用分组卷积将卷积运算分配给多个GPU上进行计算。 就如图所示，输入特征图的尺寸为$H \\times W \\times c_{1}$，输出特征图的尺寸为$H \\times W \\times c_{2}$。 若是进行正常的卷积操作（图的上半部分），那么需要使用$c2$个尺寸为$h_{1} \\times w_{1} \\times c_{1}$的卷积核，那么参数量便是$h_{1} \\times w_{1} \\times c_{1} \\times c_{2}$，这里的$h_{1} \\times w_{1}$便是卷积核的尺寸。 若是进行分组卷积操作（图的下半部分），首先需要将原输入按照通道数量先分成g个组。对于每个组来说，输入的尺寸便是$H \\times W \\times (c_{1}/g)$，相对应的每个组的输出尺寸应该是$H \\times W \\times (c_{2}/g)$。那么每个组需要使用$c2/g$个尺寸为$h_{1} \\times w_{1} \\times c_{1}/g$的卷积核，那么每组参数量便是$h_{1} \\times w_{1} \\times (c_{1}/g) \\times(c_{2}/g)$，这样的操作需要进行g次，于是，总的参数量便是$h_{1} \\times w_{1} \\times (c_{1}/g) \\times(c_{2}/g) \\times g =h_{1} \\times w_{1} \\times c_{1} \\times c_{2}/g$，也就是原来的正常卷积的$1/g$。每一组卷积的计算结果最后concat起来也就能达到c2的通道数。 本质上来看，深度可分离卷积中的深度卷积就是一种特殊的分组卷积，只是说他的分组的组数正好和输入图像的通道数一致。 2.2 点卷积（pointwise Conv） 这里的点卷积也就是深度可分离卷积中的点卷积，也就是执行1x1卷积来融合所有通道上的信息。 2.3 分组点卷积（pointwise group convolution） 论文的分组点卷积也就是说又点卷积又分组。也就是之前的分组卷积分析中，将卷积核的大小变为$1 \\times 1$的卷积。但是这里的点卷积就不是贯通输入全通道了，而是贯通分组后的每一组的全通道。 ","date":"2021-11-10","objectID":"/2021/11/shufflenet/:0:2","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["论文笔记"],"content":"3. 通道重排（Channel shuffle） 分组点卷积本身可以大幅的降低模型的参数量，但是也存在这很大的问题。也就是组和组之间没有合理的交互。每个组都是一路自己管自己走下来的，和其他组之间丝毫没有交互。信息就比较阻塞，不能流动，特征表示也不会很好。于是就引入了shuffle的概念，也就是通过shuffle的方式，希望使得组和组形成交互。 也就是将前一次分组卷积的结果，进行规则的打乱，将打乱后混合的结果再输入到下一个分组卷积之中，就如图所示，这样不同组的通道就会混合起来。具体来说，将上一次分组卷积的结果中的每一个组中的通道，这些通道再进行排列，进入到下一个分组卷积。这种不同组之间通道的混乱排列， 称为channel shuffle 。 ","date":"2021-11-10","objectID":"/2021/11/shufflenet/:0:3","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["论文笔记"],"content":"4. ShuffleNet 基于shuffle的思想，提出了两个unit，最后用来构造shufflenet。 图中，a是一种带深度可分离卷积的bottleneck结构，b和c都是所提出的模块，其中c是在降图像分辨率时使用的。 分析（b），可以看出，先做一个分组点卷积，再接上shuffle操作，然后进行一个3x3的深度可分离卷积，最后再进行一次分组点卷积。具体的实现可以看代码来进一步的分析。文章中涉及了不同的分组数（1，2，3，4，8）时的网络构架。 ","date":"2021-11-10","objectID":"/2021/11/shufflenet/:0:4","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["论文笔记"],"content":"5. ShuffleNet代码 shufflenet中代码比较有趣的点就在于shuffle的实现，如何实现shuffle就是如何实现图中打乱的部分。实际上是利用了张量维度的变化来实现这一功能。 具体来说，对于下面这个图来讲，现在是分成了三组，每个组里有四个小球，于是就通过先reshape再转置再展开的方式来实现最后的目的。 对于通道数为c的输入，先将其拆分为g组，每组有n各通道。也就是说$c = g \\times n$。 进行拆分 reshape (c=g * n) -\u003e (g, n) 进行转置 transpose (g, n) -\u003e (n, g) 将其展开 flatten 具体的shuffle步骤的代码： def shuffle_channels(x, groups): \"\"\"shuffle channels of a 4-D Tensor\"\"\" batch_size, channels, height, width = x.size() assert channels % groups == 0 ##要确保channel能被组数整除 channels_per_group = channels // groups #每个组中的通道数 # split into groups x = x.view(batch_size, groups, channels_per_group, height, width) #降通道拆分成组 # transpose 1, 2 axis x = x.transpose(1, 2).contiguous() ###进行转置 # reshape into orignal x = x.view(batch_size, channels, height, width) return x ShuffleNet中stride=1的unit： class ShuffleNetUnitA(nn.Module): \"\"\"ShuffleNet unit for stride=1\"\"\" def __init__(self, in_channels, out_channels, groups=3): super(ShuffleNetUnitA, self).__init__() assert in_channels == out_channels assert out_channels % 4 == 0 bottleneck_channels = out_channels // 4 self.groups = groups self.group_conv1 = nn.Conv2d(in_channels, bottleneck_channels, 1, groups=groups, stride=1) self.bn2 = nn.BatchNorm2d(bottleneck_channels) self.depthwise_conv3 = nn.Conv2d(bottleneck_channels, bottleneck_channels, 3, padding=1, stride=1, groups=bottleneck_channels) self.bn4 = nn.BatchNorm2d(bottleneck_channels) self.group_conv5 = nn.Conv2d(bottleneck_channels, out_channels, 1, stride=1, groups=groups) self.bn6 = nn.BatchNorm2d(out_channels) def forward(self, x): out = self.group_conv1(x)##第一个分组卷积 out = F.relu(self.bn2(out)) out = shuffle_channels(out, groups=self.groups) ##进行shuffle out = self.depthwise_conv3(out)##进行3x3的深度可分离卷积 out = self.bn4(out) out = self.group_conv5(out) ##第二个分组卷积 out = self.bn6(out) out = F.relu(x + out) return out ","date":"2021-11-10","objectID":"/2021/11/shufflenet/:0:5","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["论文笔记"],"content":"6. ShuffleNet v2 在v2中，作者主要是探索了影响模型运算速度的其他指标，如访问代价（Memory Access Cost，MAC），GPU并行计算等。并得出了一些结论并对模型进行了相对应的改进。 ① 当输入输出通道数相同时，MAC最小，运算速度最快 根据这一特性，设计了channel split，将输入通道分成两半，一边进入bottleneck，一边不操作，bottlenet里的深度可分离卷积就可以拥有相同的输入输出。最后是进行concat，而不是v1中的add。 ② 分组卷积的使用，或者分组数g越大，会导致MAC变大 根据这一特性要考虑到谨慎的使用分组卷积，防止分组卷积的过度使用。在v2中，替换了分组点卷积为普通的点卷积。 ③ 分支结构会产生碎片化会降低并行能力 对于上图的各类设计，在FLOPs数量相同的情况下，按效率从高到低为(a) \u003e (b) \u003e (d) \u003e (c) \u003e (e)，可见分支结构的效率偏差。根据这一特性，考虑取消分组卷积。 ④ Element-wise操作是耗时的 Element-wise，如ReLU激活，偏置，单位加等。 根据这一特性进行了channel split，合并的步骤变成了concat而不是相加了。 v2整体上从模型运行速度这一实际的指标展开，通过严谨的数学证明和实际的硬件表现来得出一些影响速度的因素，进一步考虑到实际运行中的各个因素。 ","date":"2021-11-10","objectID":"/2021/11/shufflenet/:0:6","tags":["轻量级网络","ShuffleNet"],"title":"【论文笔记】ShuffleNet","uri":"/2021/11/shufflenet/"},{"categories":["环境配置"],"content":"Miniconda is a free minimal installer for conda. It is a small, bootstrap version of Anaconda","date":"2021-11-06","objectID":"/2021/11/miniconda/","tags":["Miniconda","conda"],"title":"【环境配置】Miniconda-不设环境变量/北外源","uri":"/2021/11/miniconda/"},{"categories":["环境配置"],"content":"0. 写在前面 最近给服务器重装LVM，也重新配置了一下miniconda，记录一下大概的过程。 ","date":"2021-11-06","objectID":"/2021/11/miniconda/:0:1","tags":["Miniconda","conda"],"title":"【环境配置】Miniconda-不设环境变量/北外源","uri":"/2021/11/miniconda/"},{"categories":["环境配置"],"content":"1. 安装conda miniconda是精简版的anaconda，需要什么装什么，相比之下anaconda可能比较臃肿。 下载安装包 命令行下执行： cd ~/Downloads/ wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh ##下载安装包 进行安装 chmod 777 ~/Downloads/Miniconda3-latest-Linux-x86_64.sh bash ~/Downloads/Miniconda3-latest-Linux-x86_64.sh 没有特殊要求的话，直接回车就行，开始安装。 安装的过程中注意不要将conda添加进环境变量，选no，然后安装完成。 ","date":"2021-11-06","objectID":"/2021/11/miniconda/:0:2","tags":["Miniconda","conda"],"title":"【环境配置】Miniconda-不设环境变量/北外源","uri":"/2021/11/miniconda/"},{"categories":["环境配置"],"content":"2. 配置conda启动 此时，虽然miniconda已经装好了，但是并不能直接使用conda命令。【如果假如环境变量的话，每次打开终端都会显示（base）在命令的前面】。 如果需要使用conda的话，就需要执行以下命令 cd ~/miniconda3/bin chmod 777 activate source ./activate 这样的话，前面就出现了base，也就是启动了conda，自然可以里面使用conda的一系列命令。如conda list 等等 allias简化命令 如果每一次都进行这样的操作可能就比较麻烦，所以可以设置一个快捷命令 设置方法： sudo gedit ~/.bashrc #编辑.bashrc这个文件 找到如图所示的地方，加上红框出的一句话 alias condaup='. ~/miniconda3/bin/activate' 保存。再在命令行执行 source .bashrc 于是这个时候只需要执行condaup便可以在命令行里运行conda ","date":"2021-11-06","objectID":"/2021/11/miniconda/:0:3","tags":["Miniconda","conda"],"title":"【环境配置】Miniconda-不设环境变量/北外源","uri":"/2021/11/miniconda/"},{"categories":["环境配置"],"content":"3. 设置国内conda源 如果不设置国内源，在安装一些大型的包如pytorch等就安装不了。大部分网上的教程还是使用清华源，但是用的人多承载量比较大，所以可以使用北京外国语大学的镜像站点。具体操作如下： sudo gedit ~/.condarc #编辑.condarc这个文件，文件刚开始是全空的。 将下面内容复制进去并保存： channels: - https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/ - https://mirrors.bfsu.edu.cn/anaconda/pkgs/free/ - https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.bfsu.edu.cn/anaconda/cloud/bioconda/ - https://mirrors.bfsu.edu.cn/anaconda/cloud/menpo/ - https://mirrors.bfsu.edu.cn/anaconda/cloud/msys2/ - https://mirrors.bfsu.edu.cn/anaconda/cloud/pytorch/ - defaults show_channel_urls: true 保存。这个时候就能快速的安装pytorch了。 ","date":"2021-11-06","objectID":"/2021/11/miniconda/:0:4","tags":["Miniconda","conda"],"title":"【环境配置】Miniconda-不设环境变量/北外源","uri":"/2021/11/miniconda/"},{"categories":["论文笔记"],"content":"（GAN、CGAN、pix2pix、CycleGAN）","date":"2021-11-02","objectID":"/2021/11/gan/","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"写在前面 之前看到视网膜血管分割的，有很多用GAN做的。然后就主要是对GAN的一些基础知识的学习，大概了解一下gan的思想。主要就是四篇文章 GAN ,CGAN, pix2pix和 cyclegan 主要还是看代码理解的，代码也加了一定量注释 ","date":"2021-11-02","objectID":"/2021/11/gan/:1:0","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"一、【GAN】 Paper: Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. “Generative adversarial nets.” Advances in neural information processing systems 27 (2014). remark：GAN的提出 cited by： 34267 code：https://github.com/eriklindernoren/PyTorch-GAN ","date":"2021-11-02","objectID":"/2021/11/gan/:2:0","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"1. 概要 提出了一个基于对抗的新生成式模型， 它由一个生成器和一个判别器组成 生成器的目标是学习到样本的数据分布， 从而能生成样本欺骗判别器； 判别器的目标是判断输入样本是生成/真实的概率。对于任意的生成器和判别器， 都存在一个独特的全局最优解 这篇文章中，生成器和判别器都由多层感知机实现 判别式模型 • 模型学习的是条件概率分布P(Y|X) • 任务是从属性X（特征） 预测标记Y（类别） 生成式模型 • 模型学习的是联合概率分布P(X,Y) • 任务是得到属性为X且类别为Y时的联合概率 ","date":"2021-11-02","objectID":"/2021/11/gan/:2:1","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"2. 基本架构 In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms [16] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary 随机噪声作为生成器的输入，模型是一个多层感知机的结构，判别器同样也是多层感知机。 ","date":"2021-11-02","objectID":"/2021/11/gan/:2:2","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"3. VAE Variational Auto-Encoder 编码器把数据编码成mean vector和standard deviation vector 采样从构建的高斯分布中采样得到latent vector 解码器从latent vector生成数据 一个encoder-decoder的架构 generation_loss = mean(square(generated_image - real_image)) latent_loss =KL-Divergence(latent_variable,unit_gaussian) #构造的高斯分布和单位高斯分布的KL散度 loss=generation_loss +latent_loss ","date":"2021-11-02","objectID":"/2021/11/gan/:2:3","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"4. GAN 生成器 G：多层感知机, ReLU, Sigmoid 判别器 D：多层感知机, Maxout, Dropout ","date":"2021-11-02","objectID":"/2021/11/gan/:2:4","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"5. Value function $$ \\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))] $$ data: 真实数据 D: 判别器， 输出值为 [0, 1]， 代表输入来自真实数据的概率 z: 随机噪声 G: 生成器， 输出为合成数据 D的目标， 是最大化价值函数V 对数函数log在底数大于1时， 为单调递增函数 最大化V， 就是最大化 D(x) 和 1-D(G(z)) 对于任意的x， 都有 D(x) = 1 对于任意的z， 都有 D(G(z))) = 0 G的目标， 是针对特定的D， 去最小化价值函数V 最小化V， 就是最小化 D(x) 和 1-D(G(z)) 对于任意的z， 都有 D(G(z))) = 1 ","date":"2021-11-02","objectID":"/2021/11/gan/:2:5","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"6. 训练流程 • 训练k次判别器（ 论文实验中k=1） • 训练1次生成器 ","date":"2021-11-02","objectID":"/2021/11/gan/:2:6","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"7.代码 生成器和判别器均为全连接层 class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, z): img = self.model(z) img = img.view(img.size(0), *img_shape) #28*28 return img class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), #0-1 ) def forward(self, img): img_flat = img.view(img.size(0), -1) validity = self.model(img_flat) return validity 训练过程 # Loss function adversarial_loss = torch.nn.BCELoss() #对抗损失以交叉熵损失的形式出现 # Initialize generator and discriminator generator = Generator() discriminator = Discriminator() # Configure data loader os.makedirs(\"../../data/mnist\", exist_ok=True) dataloader = torch.utils.data.DataLoader( datasets.MNIST( \"../../data/mnist\", train=True, download=True, transform=transforms.Compose( [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ), ), batch_size=opt.batch_size, shuffle=True, ) # Optimizers，生成两个优化器，分别针对生成器和判别器的参数 optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) # ---------- # Training 开始训练 # ---------- for epoch in range(opt.n_epochs): for i, (imgs, _) in enumerate(dataloader): # Adversarial ground truths 对抗时候的GT, 这里1代表真，0代表假 valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) # 1 fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # 0 # Configure input real_imgs = Variable(imgs.type(Tensor)) ##这个是真的图片 # ----------------- # Train Generator 开始训练生成器（目的是使得生成的图片能以假乱真） # ----------------- optimizer_G.zero_grad() # Sample noise as generator input z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))) #生成随机向量作为输入 # Generate a batch of images gen_imgs = generator(z) ##随机向量进入生成器 1*100 --\u003e 1*5xx _\u003e 28*28 # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss(discriminator(gen_imgs), valid) # 希望能误导判别器，使得判别器认为该图是真 g_loss.backward() ##反向传播 optimizer_G.step() # --------------------- # Train Discriminator 开始训练判别器（目的是希望能正确判断真假图片） # --------------------- optimizer_D.zero_grad() # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss(discriminator(real_imgs), valid) #对于一张本来就是真的图片希望他能判断为真 fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) #对于一张假的图片希望他能判断为假 d_loss = (real_loss + fake_loss) / 2 d_loss.backward() optimizer_D.step() ","date":"2021-11-02","objectID":"/2021/11/gan/:2:7","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"二、【CGAN】 Paper: Mirza, Mehdi, and Simon Osindero. “Conditional generative adversarial nets.” arXiv preprint arXiv:1411.1784 (2014). remark：Conditional GAN, 可以生成指定条件下的图像 cited by： 5877 code：https://github.com/caffeinism/cDC-GAN-pytorch ","date":"2021-11-02","objectID":"/2021/11/gan/:3:0","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"1. 概要 在原模型基础上，会输入额外的数据作为条件，对生成器和判别器都进行了修改。例如，在MNIST数据集上， 新模型可以生成以数字类别标签为条件的手写数字图像，模型还可以用来做多模态学习，可以生成输入图像相关的描述标签 。 ","date":"2021-11-02","objectID":"/2021/11/gan/:3:1","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"2.多模态学习和图像描述 多模态学习： 图像标记： 用词语对图像中不同内容进行多维度表述 图像描述： 把一幅图片翻译为一段描述文字，获取图像的标记词语，理解图像标记之间的关系，生成人类可读的句子 ","date":"2021-11-02","objectID":"/2021/11/gan/:3:2","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"3. 网络结构 与原始GAN不同的一点就在于，加入了Y，作为一个条件输入，在生成器和判别器中的y是同一个y。这里是嵌入后concat到一起。 GAN的价值函数： $$ \\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))] $$ CGAN的价值函数： $$ \\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x} \\mid \\boldsymbol{y})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z} \\mid \\boldsymbol{y})))] $$ 两者主要的区别就在于有一个条件的形式：x|y ","date":"2021-11-02","objectID":"/2021/11/gan/:3:3","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"4. 实验 单模态任务【手写数字识别】 y一个十维的向量，代表了数字0-9，它作为条件信息输入生成器和判别器。 训练复杂：采用随机梯度下降，使用初始值为0.5的初始动量， 并逐渐增加到0.7。在生成器和判别器上都使用概率为0.5的Dropout。 使用验证集上的最大对数似然估计作为停止点 。 多模态任务【有图像有文本】 左边是在ImageNet上训练一个类似AlexNet的图像分类模型， 使用其最后一个全连接层的输出来提取图像特征。 右边是使用YFCC100M数据集， 训练一个词向量长度为200的 skip-gram模型（word2vector）。 基于MIR Flickr 25,000数据集， 使用上面的图像特征提取模型和skip-gram模型分别提取图像和标签特征。把提取的图像作为条件输入， 标签特征作为输出来训练CGAN。在训练CGAN时，不修改图像特征提取模型和skip-gram模型。在训练集内具有多个标签的图像， 每个标签训练一次。为每个条件输入生成100个样本， 对于每个样本输出的词向量，找到距离最近的20个单词。 在100*20个单词中，选择前10个最常见的单词 。 ","date":"2021-11-02","objectID":"/2021/11/gan/:3:4","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"5.代码 生成器和判别器也都是多层感知机，区别在于需要concat一个条件信息 class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes) def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim + opt.n_classes, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, noise, labels): # 1x100 1x1(0-9) # Concatenate label embedding and image to produce input 1x10 gen_input = torch.cat((self.label_emb(labels), noise), -1) #64*100 emb 64 -\u003e 64*110 img = self.model(gen_input) img = img.view(img.size(0), *img_shape) return img class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes) self.model = nn.Sequential( nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 1), ) def forward(self, img, labels): # Concatenate label embedding and image to produce input d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1) validity = self.model(d_in) return validity 训练过程 # Loss functions adversarial_loss = torch.nn.MSELoss() # Initialize generator and discriminator generator = Generator() discriminator = Discriminator() # Configure data loader os.makedirs(\"../../data/mnist\", exist_ok=True) dataloader = torch.utils.data.DataLoader( datasets.MNIST( \"../../data/mnist\", train=True, download=True, transform=transforms.Compose( [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ), ), batch_size=opt.batch_size, shuffle=True, ) # Optimizers optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) def sample_image(n_row, batches_done): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim)))) # Get labels ranging from 0 to n_classes for n rows labels = np.array([num for _ in range(n_row) for num in range(n_row)]) labels = Variable(LongTensor(labels)) gen_imgs = generator(z, labels) save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True) # ---------- # Training # ---------- for epoch in range(opt.n_epochs): for i, (imgs, labels) in enumerate(dataloader): batch_size = imgs.shape[0] # Adversarial ground truths valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False) fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(FloatTensor)) labels = Variable(labels.type(LongTensor)) #是数字，代表上面的img是什么数字 # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise and labels as generator input 生成随机向量和条件作为输入 z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim)))) #64*100 gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size))) #64 # Generate a batch of images gen_imgs = generator(z, gen_labels) #64*1*32*32 # Loss measures generator's ability to fool the discriminator validity = discriminator(gen_imgs, gen_labels) # 64*1 g_loss = adversarial_loss(validity, valid) ##希望是真 g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Loss for real images validity_real = discriminator(real_imgs, labels) d_real_loss = adversarial_loss(validity_real, valid) # Loss for fake images validity_fake =","date":"2021-11-02","objectID":"/2021/11/gan/:3:5","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"三、【pix2pix】 Paper: Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. “Image-to-image translation with conditional adversarial networks.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125-1134. 2017. remark：图像翻译 cited by： 10095 code：https://phillipi.github.io/pix2pix/?utm_source=catalyzex.com ","date":"2021-11-02","objectID":"/2021/11/gan/:4:0","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"1. 概要 研究条件生成式对抗网络在图像翻译任务中的通用解决方案。网络不仅学习从输入图像到输出图像的映射（生成器），还学习了用于训练该映射的损失函数（判别器）。把这种方法可以有效应用在图像合成，图像上色等多种图像翻译任务中。表明可以在不手工设计损失函数的情况下，也能获得理想的结果。 ","date":"2021-11-02","objectID":"/2021/11/gan/:4:1","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"2. 网络结构 生成器是一个UNet。 判别器是PatchGAN 作者认为像素级的l1 loss能很好的捕捉到图像中的低频信息，GAN的判别器只需要关注高频信息。所以把图像切成 N*N 的patch，其中N显著小于图像尺寸。假设在大于N时，像素之间是相互独立的，从而可以把图像建模成马尔科夫随机场。把判别器在所有patch上的推断结果，求平均来作为最终输出。可以把PatchGAN理解为对图像纹理/style损失的计算。 目标函数 总的目标是: $$ G^{*}=\\arg \\min _{G} \\max _{D} {L}_{c G A N}(G, D)+\\lambda {L}_{L 1}(G) . $$ 它由一个cgan损失和L1损失加权相加而成。其中cgan的损失为： $$ \\begin{aligned} \\mathcal{L}_{c G A N}(G, D)=\u0026 \\mathbb{E}_{x, y}[\\log D(x, y)]+\\mathbb{E}_{x, z}[\\log (1-D(x, G(x, z))] \\end{aligned} $$ 这里的x是条件，也就是一个分割图。y是通过生成器生成的实景图。在判别器中，y可以是前面生成器的输出，也可以是GT。patchgan的体现就是，最后输出时，(16,16,1)中的每一个像素点，都代表着原图中的一个16x16的patch。 criterion_GAN = torch.nn.MSELoss() criterion_pixelwise = torch.nn.L1Loss() for epoch in range(opt.epoch, opt.n_epochs): for i, batch in enumerate(dataloader): # Model inputs real_A = Variable(batch[\"B\"].type(Tensor)) #真实的分割图 1*3*256*256 real_B = Variable(batch[\"A\"].type(Tensor)) #真实的建筑图 1*3*256*256 # Adversarial ground truths valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False) #全1(1*1*16*16) fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False) #全0(1*1*16*16) # ------------------ # Train Generators # ------------------ optimizer_G.zero_grad() # GAN loss fake_B = generator(real_A) #先通过真的分割图生成假的建筑图 1*3*256*256 pred_fake = discriminator(fake_B, real_A) #判别一个这个假的建筑图 1*1*16*16 loss_GAN = criterion_GAN(pred_fake, valid) #希望被判别器识别错误 # Pixel-wise loss loss_pixel = criterion_pixelwise(fake_B, real_B) #计算与真实建筑图之间的差距 1*3*256*256 # Total loss loss_G = loss_GAN + lambda_pixel * loss_pixel loss_G.backward() optimizer_G.step() #先训练生成器，对于生成器来讲，有两个损失GAN loss（由MSE实现）和Pixel-wise loss。其中GAN loss就是希望生成器生成的假图片逼近真的。Pixel-wise loss就是生成图和label的L1—loss。 # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Real loss pred_real = discriminator(real_B, real_A) loss_real = criterion_GAN(pred_real, valid) # Fake loss pred_fake = discriminator(fake_B.detach(), real_A) loss_fake = criterion_GAN(pred_fake, fake) # Total loss loss_D = 0.5 * (loss_real + loss_fake) loss_D.backward() optimizer_D.step() ##对于判别器来讲，一方面希望能将真实的图片识别为真。另外一方面，希望将假的图片识别为假，两个平均求和 # -------------- # Log Progress # -------------- # Determine approximate time left batches_done = epoch * len(dataloader) + i batches_left = opt.n_epochs * len(dataloader) - batches_done time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time)) prev_time = time.time() ","date":"2021-11-02","objectID":"/2021/11/gan/:4:2","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"四、【CycleGAN】 Paper: Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A. Efros. “Unpaired image-to-image translation using cycle-consistent adversarial networks.” In Proceedings of the IEEE international conference on computer vision, pp. 2223-2232. 2017. remark：图像翻译 无监督 Domain Adaptation cited by： 9546 code：https://github.com/junyanz/CycleGAN ","date":"2021-11-02","objectID":"/2021/11/gan/:5:0","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"1. 概要 一般来说，图像翻译任务需要对齐的图像对， 但很多场景下无法获得这样的训练数据。于是作者提出了一个基于非配对数据的方法， 可以学习到不同 domain 图像间的映射。CycleGAN是在GAN loss的基础上加入循环一致性损失，使得 F(G(X)) 尽量接近X（反之亦然）。 ","date":"2021-11-02","objectID":"/2021/11/gan/:5:1","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"2. 网络结构和设计框架 生成器 判别器 判别器使用了PatchGAN 架构 ","date":"2021-11-02","objectID":"/2021/11/gan/:5:2","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["论文笔记"],"content":"3. 损失函数 目标是在X和Y两个不同domain间，建立起双向的映射关系 G 和 F ；并使用两个判别器$D_X$和$D_Y$，来分别对{x}和{F(Y)}、{y}和{G(x)}进行区分，于是就存在两个损失： • 对抗损失——使得映射后的数据分布接近目标domain的数据分布 • 循环一致性损失——保证学习到的两个映射 G 和 F 不会相互矛盾 GAN损失使用的是和传统GAN网络一致的对抗损失函数 $$ \\begin{aligned} \\mathcal{L}_{\\mathrm{GAN}}\\left(G, D_{Y}, X, Y\\right) \u0026=\\mathbb{E}_{y \\sim p_{\\text {atat }}(y)}\\left[\\log D_{Y}(y)\\right] \\\\ \u0026+\\mathbb{E}_{x \\sim p_{\\text {data }}(x)}\\left[\\log \\left(1-D_{Y}(G(x))\\right]\\right. \\end{aligned} $$ 优化目标是两个min-max函数 $$ \\begin{aligned} \u0026\\min _{G} \\max _{D_{Y}} \\mathcal{L}_{\\mathrm{GAN}}\\left(G, D_{Y}, X, Y\\right) \\\\ \u0026\\min _{F} \\max _{D_{X}} \\mathcal{L}_{\\mathrm{GAN}}\\left(F, D_{X}, Y, X\\right) \\end{aligned} $$ 循环一致性损失 ，对于任意一个x和y， 应该有： $$ \\begin{aligned} \u0026x \\rightarrow G(x) \\rightarrow F(G(x)) \\approx x \\\\ \u0026y \\rightarrow F(y) \\rightarrow G(F(y)) \\approx y \\end{aligned} $$ 使用L1距离时， 则损失函数为： $$ \\begin{aligned} \\mathcal{L}_{\\text {cyc }}(G, F) \u0026=\\mathbb{E}_{x \\sim p_{\\text {data }}(x)}\\left[\\|F(G(x))-x\\|_{1}\\right] \\\\ \u0026+\\mathbb{E}_{y \\sim p_{\\text {data }}(y)}\\left[\\|G(F(y))-y\\|_{1}\\right] \\end{aligned} $$ 于是，完整的损失函数应该为： $$ \\begin{aligned} \\mathcal{L}\\left(G, F, D_{X}, D_{Y}\\right) \u0026=\\mathcal{L}_{\\mathrm{GAN}}\\left(G, D_{Y}, X, Y\\right) \\\\ \u0026+\\mathcal{L}_{\\mathrm{GAN}}\\left(F, D_{X}, Y, X\\right) \\\\ \u0026+\\lambda \\mathcal{L}_{\\text {cyc }}(G, F) \\end{aligned} $$ 论文里没有提，但是代码中还存在的一个损失，identity损失： $$ \\begin{aligned} \\mathcal{L}_{\\text {identity }}(G, F)=\\mathbb{E}_{y \\sim p_{\\text {data }}(y)}\\left[\\|G(y)-y\\|_{1}\\right]+ \u0026\\mathbb{E}_{x \\sim p_{\\text {data }}(x)}\\left[\\|F(x)-x\\|_{1}\\right] \\end{aligned} $$ #先定义损失函数： criterion_GAN = torch.nn.MSELoss() ##判别器损失 criterion_cycle = torch.nn.L1Loss() ##循环一致性损失 criterion_identity = torch.nn.L1Loss() ##identity损失 #两个生成器 G_AB = GeneratorResNet(input_shape, opt.n_residual_blocks) G_BA = GeneratorResNet(input_shape, opt.n_residual_blocks) #两个判别器 D_A = Discriminator(input_shape) D_B = Discriminator(input_shape) ##定义优化器 optimizer_G = torch.optim.Adam( itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2) ) optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) for epoch in range(opt.epoch, opt.n_epochs): for i, batch in enumerate(dataloader): # Set model input real_A = Variable(batch[\"A\"].type(Tensor)) # A是油画图 1*3*256*256 real_B = Variable(batch[\"B\"].type(Tensor)) # B是真实的风景图 1*3*256*256 # Adversarial ground truths valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)#表真 #1*1*16*16 fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)#表假 # ------------------ # Train Generators # ------------------ ##训练生成器： G_AB.train() G_BA.train() optimizer_G.zero_grad() #生成器的优化器 # Identity loss loss_id_A = criterion_identity(G_BA(real_A), real_A) #通过BA生成器后的输出和自身的损失，不要偏离太远 loss_id_B = criterion_identity(G_AB(real_B), real_B) # 1*3*256*256 loss_identity = (loss_id_A + loss_id_B) / 2 # GAN loss fake_B = G_AB(real_A) # 1*3*256*256 loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) #对于生成器来讲，希望生成的假图被判别器判断为真。 D_B(fake_B)的结果是1*1*16*16 fake_A = G_BA(real_B) loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) #A到B，B到A各来一次 loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2 # Cycle loss recov_A = G_BA(fake_B) # 1*3*256*256 loss_cycle_A = criterion_cycle(recov_A, real_A) #生成的假图再从B-\u003eA，得到循环一致性损失 recov_B = G_AB(fake_A) loss_cycle_B = criterion_cycle(recov_B, real_B) loss_cycle = (loss_cycle_A + loss_cycle_B) / 2 # Total loss loss_G = loss_GAN + opt.lambda_cyc * loss_cycle + opt.lambda_id * loss_identity loss_G.backward() ###同时优化两个生成器 optimizer_G.step() ##训练判别器A： #两个判别器是分开来训练更新的 # ----------------------- # Train Discriminator A # ----------------------- optimizer_D_A.zero_grad() # Real loss loss_real = criterion_GAN(D_A(real_A), valid) #1*1*16*16 # Fake loss (on batch of previously generated samples) fake_A_ = fake_A_buffer.push_and_pop(fake_A) loss_fake = criterion_GAN(D_A(fake_A_.","date":"2021-11-02","objectID":"/2021/11/gan/:5:3","tags":["GAN","生成对抗网络"],"title":"【论文笔记】GAN基础与代码解读（GAN、CGAN、pix2pix、CycleGAN）","uri":"/2021/11/gan/"},{"categories":["Linux学习"],"content":"LVM是为逻辑卷管理，它是Linux下对磁盘分区进行管理的一种机制。","date":"2021-10-31","objectID":"/2021/10/lvm/","tags":["Linux","LVM"],"title":"【Linux学习】Linux-LVM","uri":"/2021/10/lvm/"},{"categories":["Linux学习"],"content":"0.写在前面 服务器硬盘不够了，但是发现挂载似乎必须要挂载到新的文件系统上，发现了这么一个技术，试了一试。说白了，就是通过逻辑的方式来管理硬盘，可以动态的给逻辑区域增加和减少空间，更加方便。 LVM 是 Logical Volume Manager （逻辑卷管理），是一种动态磁盘管理机制。LVM 是建立在磁盘分区和文件系统之间的一个逻辑层。相当于把底层的封装起来，用逻辑卷来管理硬盘。管理员可以利用 LVM 在不重新对磁盘分区的情况下动态的调整分区的大小。如果系统新增了一块硬盘，通过 LVM 就可以将新增的硬盘空间直接扩展到原来的磁盘分区上。 ","date":"2021-10-31","objectID":"/2021/10/lvm/:0:1","tags":["Linux","LVM"],"title":"【Linux学习】Linux-LVM","uri":"/2021/10/lvm/"},{"categories":["Linux学习"],"content":"1.几个概念 物理块（Physical Extent，PE） PE是物理卷PV的基本划分单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可配置的，默认为4MB。所以物理卷（PV）由大小等同的基本单元PE组成。 物理卷（Physical Volume，PV） 指磁盘分区或从逻辑上与磁盘分区具有同样功能的设备（如RAID），是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数。 卷组（Volume Group，VG） 类似于非LVM系统中的物理磁盘，其由一个或多个物理卷PV组成。可以在卷组上创建一个或多个LV（逻辑卷）。 逻辑卷（Logical Volume，LV） 类似于非LVM系统中的磁盘分区，逻辑卷建立在卷组VG之上。在逻辑卷LV之上可以建立文件系统（比如/home或者/usr等）。 所有操作的目的都是为了创建逻辑卷出来 ","date":"2021-10-31","objectID":"/2021/10/lvm/:0:2","tags":["Linux","LVM"],"title":"【Linux学习】Linux-LVM","uri":"/2021/10/lvm/"},{"categories":["Linux学习"],"content":"2.步骤 1.先将已有的物理硬盘（比如sda）进行条带化成物理卷PV（相当于把物理硬盘划分成了一个一个PE，PE默认是4M大小） 2.创建一个卷组VG，相当于是一个空间池，可以将多个PV加入到VG之中。 3.基于VG中的PE创建最终要使用的逻辑卷LV 4.LV可以直接格式化挂载使用 5.LV的扩充和缩减实际上就是增加和减少组成改LV的PE的数量，过程中不会丢失原始数据 最后出现的逻辑卷的设备名字：/dev/vgname/lvname，最后是对这个东西进行格式化挂载。 对于一个逻辑卷LV中可以来自不同的物理设备的。 VG不够用的时候，就往VG卷组里加硬盘就行 ","date":"2021-10-31","objectID":"/2021/10/lvm/:0:3","tags":["Linux","LVM"],"title":"【Linux学习】Linux-LVM","uri":"/2021/10/lvm/"},{"categories":["Linux学习"],"content":"3.LVM创建并使用 1.将物理磁盘设备初始化为物理卷 pvcreate /dev/sda /dev/sdb 2.创建卷组，并将PV将入卷组vg1中 vgcreate vg1 /dev/sda /dev/sdb 3.基于卷组创建逻辑卷 lvcreate -n lv1 -L 2G vg1 从vg1这个卷组中，抽出2G来建立lv1这个逻辑卷 4.为创建好的逻辑卷创建文件系统 mkfs.ext4 /dev/vg1/lv1 5.将格式化好的逻辑卷挂载使用 mount /dev/vg1/lv1 /mnt 信息查看 查看物理卷信息：pvdisplay / pvs 查看卷组信息：vgdisplay / vgs 查看逻辑卷信息：lvdisplay / lvs 删除 删除LV lvremove /dev/vg1/lv1 删除VG vgremove vg1 删除物理卷 pvremove /dev/sda ","date":"2021-10-31","objectID":"/2021/10/lvm/:0:4","tags":["Linux","LVM"],"title":"【Linux学习】Linux-LVM","uri":"/2021/10/lvm/"},{"categories":["Linux学习"],"content":"4.LVM扩大缩小 拉伸逻辑卷（可以在线执行） 1.查看VG中还有空间 vgdisplay 2.扩充逻辑卷 lvextend -L +1G /dev/vg1/lv1 3.查看扩充后LV大小 lvdisplay 4.更新文件系统 resize2fs /dev/vg1/lv1 5.查看更新后的文件系统 df -h 拉伸卷组（可以在线执行） 1.将要添加进来的硬盘格式化为PV pvcreate /dev/sdc 2.将新的PV添加到卷组中 vgextend vg1 /dev/sdc 3.查看扩充后VG的大小 vgdisplay 缩小逻辑卷（需要离线执行） 1.卸载已经挂载的逻辑卷 umount /dev/vg1/lv1 2.缩小文件系统 resize2fs /dev/vg1/lv1 1G 3.缩小LV lvreduce -L -1G /dev/vg1/lv1 4.查看缩小后的LV lvdisplay 5.挂载 mount /dev/vg1/lv1 /mnt ","date":"2021-10-31","objectID":"/2021/10/lvm/:0:5","tags":["Linux","LVM"],"title":"【Linux学习】Linux-LVM","uri":"/2021/10/lvm/"},{"categories":["论文笔记"],"content":"MobileNets是为移动和嵌入式设备提出的高效模型，包括v1-v3","date":"2021-10-28","objectID":"/2021/10/mobilenet/","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["论文笔记"],"content":"v1 Paper: Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. “Mobilenets: Efficient convolutional neural networks for mobile vision applications.” arXiv preprint arXiv:1704.04861 (2017). remark：轻量级重要backbone google cited by： 10124 v2 Paper: Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. “Mobilenetv2: Inverted residuals and linear bottlenecks.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510-4520. 2018. remark：v1上的改进，引入残差 cited by： 6701 v3 Paper: Howard, Andrew, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang et al. “Searching for mobilenetv3.” In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314-1324. 2019. remark：引入了NAS，SE，h-swish cited by： 1282 ","date":"2021-10-28","objectID":"/2021/10/mobilenet/:0:0","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["论文笔记"],"content":"1. 概要 模型的主要动机还是在于在希望在网络的大小和速度上能更高效，再者现实生活中很多嵌入式设备的算力不足以支撑大型的网络，不能支持实时计算。 轻量研究的路径： 模型压缩：先训练好一个网络，再通过相关方法使得模型变小（蒸馏、减枝等等） 直接训练一个小网络（mobilenet、shufflenet等等对网络结构的设计） MobileNets 主要基于深度可分离卷积构成，通过设置两个超参数（宽度和分辨率），平衡了准确率和速度。 ","date":"2021-10-28","objectID":"/2021/10/mobilenet/:0:1","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["论文笔记"],"content":"2. MobileNet结构 深度可分离卷积的设计：每一次深度卷积或者点卷积的后面，都要跟上BN和ReLU。 ","date":"2021-10-28","objectID":"/2021/10/mobilenet/:0:2","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["论文笔记"],"content":"3. 深度可分离卷积 深度可分离卷积是moblienet中最为至关重要的部分，它是控制模型走向”轻量“的关键。 深度可分离卷积（Depthwise Separable Convolution，DSC）的过程可以分为深度卷积（Depthwise Convolution）和点卷积（Pointwise Convolution）两个部分。他的示意图为： 举个例子 假设现有一个输入的尺寸为CxHxW，C为通道数，h和w为图像尺寸。希望通过卷积操作得到一个NxHxW的输出，也就是说希望去改变通道数量。假设使用3x3的卷积核尺寸，那么对比普通卷积和DSC来看: 若是普通卷积，就需要N个卷积核为3x3xC的卷积核来完成这个操作，所涉及的卷积核参数量便是3x3xCxN = 9CN。 若是深度可分离卷积，对于输入的每一个通道都分别使用一个3x3的卷积核，便是C个3x3的卷积核（这一步就是深度卷积，涉及的参数量是Cx3x3）。此时得到输出其实是和输入一样的，若要得到通道为N的输出，就再使用一次1x1卷积操作进行变通道到N通道（这一步就是点卷积的操作，涉及的参数量是Nx1x1）。于是总的参数量便是Cx3x3+Nx1x1=9C+N « 9CN 深度卷积对每一个通道只使用了一个卷积核，所以每一个通道都会对应的去输出一个通道。而点卷积的作用就在于将它们进行融合，这个融合的过程中，可能是提升也可能是降低这个通道的数量。深度卷积用来滤波，点卷积用来融合。 3.1 从参数量、计算量和速度来对比普通卷积和深度可分离卷积 计算复杂度–普通卷积 首先定义$D_k$是卷积核的尺寸，输入特征的尺寸为$M \\times D_{F} \\times D_{F}$，其中$M$为输入的通道数，$D_{F}$为输入特征图的尺寸，输出通道数为$N$ 于是对于一个普通卷积来讲，进行一个卷积核处理（也就是生成输出中的一个通道）它的计算量为： $$ D_{k} \\times D_{k} \\times M \\times D_{F} \\times D_{F} $$ 因为最后需要生成的通道数量为N，所以需要进行N个卷积核的处理，于是计算量就是： $$ D_{k} \\times D_{k} \\times N \\times M \\times D_{F} \\times D_{F} $$ 计算复杂度–深度可分离卷积 对于深度可分离卷积来讲，分为深度卷积和点卷积。在深度卷积中，相当于对于M通道中的每一个通道执行一次核为$1 \\times D_k \\times D_K$的卷积，其计算量为 $$ D_{k} \\times D_{k} \\times 1 \\times D_{F} \\times D_{F} \\times M $$ 在点卷积中，用N个$M \\times 1 \\times 1$的卷积核来进行操作，可以得到N通道的输出，计算量为： $$ 1 \\times 1 \\times N \\times D_{F} \\times D_{F} \\times M $$ 于是总的计算量为： $$ D_{k} \\times D_{k} \\times D_{F} \\times D_{F} \\times M + N \\times D_{F} \\times D_{F} \\times M = D_{F} \\times D_{F} \\times M (D_{k} \\times D_{k} + N ) $$ 计算复杂度对比 $$ \\begin{aligned} \u0026 \\frac{D_{K} \\cdot D_{K} \\cdot M \\cdot D_{F} \\cdot D_{F}+M \\cdot N \\cdot D_{F} \\cdot D_{F}}{D_{K} \\cdot D_{K} \\cdot M \\cdot N \\cdot D_{F} \\cdot D_{F}} \\ =\u0026 \\frac{1}{N}+\\frac{1}{D_{K}^{2}} \\approx \\frac{1}{D_{K}^{2}} \\end{aligned} $$ 如果使用3x3的卷积核，那么，计算复杂度的比值基本在1/9到1/8之间。 参数量–普通卷积 参数量主要是从卷积核的参数量来看。 对于普通卷积来讲，需要N个$M \\times D_{k} \\times D_{k}$的卷积核来生成输出，所以参数总量为： $$ N \\times M \\times D_{k} \\times D_{k} $$ 参数量–深度可分离卷积 对于深度卷积来讲，需要M个$1 \\times D_{k} \\times D_{k}$卷积核，参数量为： $$ M \\times D_{k} \\times D_{k} $$ 对于点卷积来讲，需要N个$M \\times 1 \\times 1$卷积核，参数量为： $$ N \\times M \\times 1 \\times 1=N \\times M $$ 总的参数量为： $$ M \\times D_{k} \\times D_{k}+N \\times M $$ 参数量对比 常规卷积参数量和深度可分离卷积参数量的比值： $$ \\frac{N \\times M \\times D_{k} \\times D_{k}}{M \\times D_{k} \\times D_{k}+N \\times M}=\\frac{1}{\\frac{1}{N}+\\frac{1}{D_{k}^{2}}} \\approx D_{k}^{2} $$ 所以，一般也是9倍左右。 3.2 关于速度（Roofline Model） 在很多的gpu实验中，可能会发现mobilenet的速度比vgg还要慢，虽然参数量很小。但是在cpu上似乎moblie又能快。 模型的运行速度不仅仅取决于模型本身，也与平台有很大的关系。并且有一套理论可以去解读它(Roofline Model) Roofline Model 提出了使用 Operational Intensity（计算强度）进行定量分析的方法，并给出了模型在计算平台上所能达到理论计算性能上限公式。 具体可见：https://zhuanlan.zhihu.com/p/34204282 普通卷积 深度可分离卷积 比值 计算量 $D_{k} \\times D_{k} \\times N \\times M \\times D_{F} \\times D_{F}$ $D_{F} \\times D_{F} \\times M (D_{k} \\times D_{k} + N )$ $D_{k}^{2}$ 参数量 $N \\times M \\times D_{k} \\times D_{k}$ $M \\times D_{k} \\times D_{k}+N \\times M$ $D_{k}^{2}$ 速度 Roofline Model ，取决于平台 Roofline Model ，取决于平台 ","date":"2021-10-28","objectID":"/2021/10/mobilenet/:0:3","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["论文笔记"],"content":"4. 关于MobileNet的超参数 mobilenet通过超参数的设定来控制模型的大小 4.1 宽度超参数 宽度超参数$\\alpha$用于规范每一层channel数量的设置，常取[1, 0.75, 0.5, 0.25] 算力消耗降为原来的$\\alpha^2$倍。 ####　4.2 分辨率超参数 分辨率超参数$\\rho$，也就是将原来的输入图的尺寸降到$\\rho \\times DF$ 算力消耗降为原来的$\\rho^2$倍。 ","date":"2021-10-28","objectID":"/2021/10/mobilenet/:0:4","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["论文笔记"],"content":"5. MobileNet的改进v2和v3 ５.1 MobileNet v2 针对v1，v2的改进主要集中在两个方面，即Linear Bottleneck和Inverted Residuals。 ①Linear Bottleneck 总结来说作者认为，对于浅层特征来讲，也就是特征通道数量较少的时候，使用ReLU激活会不好，这种非线性激活应该在高维度的情况下使用，就像图中所表示的，当维度小的时候（dim=2,3），relu的破坏性比较强。所以，作者在浅层的时候就考虑使用线性变化来代替非线性激活防止破坏浅层信息，于是就是所谓的linear bottleneck。其实这里的线性操作就是不带ReLU的1x1的卷积层 ②Inverted Residuals 对于resnet中的残差连接来讲，设计的是一个1x1-\u003e3x3-\u003e1x1的流程，第一个1x1的卷积是为了降通道然后再进行3x3的特征提取卷积，最后通过1x1卷积恢复。这样可以减少参数量。而在mobilenetv2中，第一个1x1卷积是为了加深通道的，这就是逆的说法。 包括可以发现，深度可分离卷积的第二个1x1卷积后面是不加relu的，3x3卷积后面加的是ReLU6（\u003e6为6）。 整体就是由这些bottleneck块来组成： ５.2 MobileNet v3 v3在v1和v2的基础上进行了改进。NAS的确不懂，等待学习，就把能看懂的改进先写一下吧~~ v1和v2的基础：依然使用了v1中的深度可分离卷积；依然使用v2中的倒残差结构。 改进：使用了NAS？（待学习）；引入了SE 注意力block；使用激活函数h-swish ①SE block可见 https://zhuanlan.zhihu.com/p/334349672 ②h-swish基于swish的： swish的公式为： $$ f(x)=x \\cdot \\operatorname{sigmoid}(\\beta x) $$ swish的作者提出swish比relu更好，但是sigmoid在移动端可能不是很友好，于是就改进为h-swish，也就是用relu6代替sigmoid： $$ \\mathrm{h}-\\operatorname{swish}[x]=x \\frac{\\operatorname{ReLU} 6(x+3)}{6} $$ 从图中也可以看出，已经非常的近似了。 感觉目前很多模块验证轻量级效果的时候，都是使用的mobilev3，可见还是有很多的实际应用场景的。 ","date":"2021-10-28","objectID":"/2021/10/mobilenet/:0:5","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["论文笔记"],"content":"6.代码 v1中深度可分离卷积： #深度卷积，一个卷积核对应一个输入特征通道，分组卷积 class dw_conv(nn.Module): def __init__(self, in_dim, out_dim, stride): super(dw_conv, self).__init__() self.dw_conv_k3 = nn.Conv2d( in_dim, out_dim, kernel_size=3, stride=stride, groups=in_dim, bias=False) #深度卷积是通过分组卷积来实现的，组的数量就是输入通道数量，这样就能对应每一个通道进行各自的卷积了。 self.bn = nn.BatchNorm2d(out_dim) self.relu = nn.ReLU(inplace=True) def forward(self, x): x = self.dw_conv_k3(x) x = self.bn(x) x = self.relu(x) return x #逐点卷积 卷积核size为1x1 class point_conv(nn.Module): def __init__(self, in_dim, out_dim): super(point_conv, self).__init__() self.p_conv_k1 = nn.Conv2d(in_dim, out_dim, kernel_size=1, bias=False) ##直接通过1x1的卷积来实现点卷积 self.bn = nn.BatchNorm2d(out_dim) self.relu = nn.ReLU(inplace=True) def forward(self, x): x = self.p_conv_k1(x) x = self.bn(x) x = self.relu(x) return x v2-v3中的InvertedResidual class InvertedResidual(nn.Module): def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs): super(InvertedResidual, self).__init__() assert stride in [1, 2] self.identity = stride == 1 and inp == oup if inp == hidden_dim: #不变通道 self.conv = nn.Sequential( # dw nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False), nn.BatchNorm2d(hidden_dim), h_swish() if use_hs else nn.ReLU(inplace=True), #v3中使用h_swish() 激活 # Squeeze-and-Excite SELayer(hidden_dim) if use_se else nn.Identity(), #v3中引入的se激活块 # pw-linear nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), ##这一步后面是不加激活函数的 nn.BatchNorm2d(oup), ) else: self.conv = nn.Sequential( #变通道 # pw nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), nn.BatchNorm2d(hidden_dim), h_swish() if use_hs else nn.ReLU(inplace=True), #v3中使用h_swish() 激活 # dw nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False), nn.BatchNorm2d(hidden_dim), # Squeeze-and-Excite SELayer(hidden_dim) if use_se else nn.Identity(), #v3中引入的se激活块 h_swish() if use_hs else nn.ReLU(inplace=True), # pw-linear nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), ##这一步后面是不加激活函数的 nn.BatchNorm2d(oup), ) def forward(self, x): if self.identity: return x + self.conv(x) else: return self.conv(x) ## 激活函数 class h_sigmoid(nn.Module): def __init__(self, inplace=True): super(h_sigmoid, self).__init__() self.relu = nn.ReLU6(inplace=inplace) def forward(self, x): return self.relu(x + 3) / 6 class h_swish(nn.Module): def __init__(self, inplace=True): super(h_swish, self).__init__() self.sigmoid = h_sigmoid(inplace=inplace) def forward(self, x): return x * self.sigmoid(x) ","date":"2021-10-28","objectID":"/2021/10/mobilenet/:0:6","tags":["轻量级网络","MobileNet"],"title":"【论文笔记】MobileNet","uri":"/2021/10/mobilenet/"},{"categories":["Python"],"content":"NumPy(Numerical Python)是Python语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。","date":"2021-10-25","objectID":"/2021/10/numpy/","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"写在前面 虽然平时都有通过查查搜索引擎来用到这些库，但是也没有一个系统的学习。想着花点时间系统看一看几个python常见库的使用。 numpy的学习主要是借鉴了一些公众号文章和一本书《利用python进行数据分析》来进行的。这书还是挺不错的，numpy、pandas、matplotlib都讲，它的作者是Pandas库的核心开发者，相当于是官方教程的感觉了。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:1:0","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"1. 数组创建和存用 numpy生成的数据结构是array，也就是数组，在内存中是连续开辟一段空间进行储存的。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:2:0","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"1.1 数组的创建 数组的创建主要是三种方式 ① 直接写上什么数组： np.array() np.array([1,2,3,4,5]) #也就是所见即所得，写了什么就是什么，往np.array()里面放个列表 ②定点定长的创建 # np.arange(start, stop, step) 固定长度元素，默认步长是1，python中大部分都是左闭右开的 print( np.arange(8) ) print( np.arange(1,8,2)) [0 1 2 3 4 5 6 7] [1 3 5 7] # np.linspace(start, stop, num) 固定元素数量，默认数量为50，这个是保尾部的 print( np.linspace(2,6,3) ) print( np.linspace(3,8,11) ) [2. 4. 6.] [3. 3.5 4. 4.5 5. 5.5 6. 6.5 7. 7.5 8. ] ③各类特殊的创建 用 np.zeros() 创建元素全是 0 的数组 用 np.ones() 创建元素全是 1 的数组 用 np.random() 创建随机元素的数组 用 np.eye() 创建对角线都是 1 其他元素都是 0 的二维数组 ","date":"2021-10-25","objectID":"/2021/10/numpy/:2:1","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"1.2 数组的储存和读取 npy格式 numpy中数组默认的存储格式是npy格式的，可以直接存储方式为： np.save(npy_file,arr) 加载它就直接用： np.load( npy_file ) txt格式 npy格式的东西，在windows下是不能直接打开查看的。除此之外，numpy中的数组还可以存为txt文本格式。 用 np.savetxt() 函数将 numpy 数组保存为 txt 文本格式： np.savetxt( txt_file, arr ) 加载用： np.loadtxt( txt_file ) 相比npy格式的，多了txt命令。 csv格式 还有日常中很多见到的可以用excel打开的csv格式，他的读取： np.genfromtxt('arr_csv.csv', delimiter=';') ### delimiter 如果数据中是有分割符的，是需要用分隔符隔开才能读取的。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:2:2","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"2. 数组在内存中的情况 数组在内存中理论上就是连续保存的。在numpy中所建立的数组也是一样。 如果是一维数组，那就是正常的直接排序就行，例如np.arange(24)生成的数组，总有24个元素。 它实际在内存中就是只有一个轴，并按照顺序进行排列。 如果把这个一维的reshape成二维np.arange(24).reshape((4,6))，它打印出来的样子是： 它在内存中的排布： 它在内存中的排布实际上也是按照顺序的，但是现在轴1之间的跨度是紧挨着的，轴0之间的跨度是有6： 如果再进一步变为三维np.arange(24).reshape((3,2,4))，打印出来是： 实际的内存排布： 此时，轴2的跨度的是最近的，轴0的跨度是最远的。 所以可以说，不管对于多高维度，最高维度的数之间是紧挨着的，最低维度的数之间离得最远： 对于numpy中的数组来讲，有一个属性，arr.strides就是来描绘，不同维度的数据的一个跨度，对于上面这个三维数组来讲，他的strides就是(32, 16, 4)，因为一个整形占四个字节，所以从对轴2来讲，跨度只有四个字节，也就是一个数，对轴0来讲，跨度就是32/4=8个数。与图中所描绘的一致。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:3:0","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"3. 数组的索引和切片 ","date":"2021-10-25","objectID":"/2021/10/numpy/:4:0","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"3.1 数组索引 索引和切片的区别： 切片是按一定规则获取子数组 索引是一个一个的获取元素 arr = [0,1,2,3,4] arr[3] #3 arr[[1,2,3]] #array([1, 2, 3]) 如果是多维数组的索引： arr2d[0][1]或者arr2d[0, 1]，这两者是等价的。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:4:1","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"3.2 数组切片 用 arr[1:4] 切片第 2 到 4个元素 (切片含头不含尾，所以没有第5个元素)。 对于上面的arr，这里的结果就是 # array([1, 2, 3]) numpy中的切片类似数组中的切片，最主要的不同在于，array的切片是一个视图的做法，也就是说，对切片的操作，会对数组本身产生改变。 如果希望不发生改变的话，可以使用 .copy来复制一份 对于多维数组来讲，切片就要分开不同的维度来进行，对每一维度的切片情况。 如果某一维度不切，想全部获取，就用冒号:，维度与维度之间用逗号隔开。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:4:2","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"3.3 布尔操作 也就是通过逻辑True和False来筛选数组。例如有两个数组arr1和arr2： arr1 = np.array ([0,0,0,1,1,1]) arr2 = np.array ([1,2,3,4,5,6]) arr2[arr1 == 0] 这样切出来的结果就是 [1,2,3] ","date":"2021-10-25","objectID":"/2021/10/numpy/:4:3","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"4. 数组的变形和拆并 一般可以用arr.shape来查看数组的形状 ","date":"2021-10-25","objectID":"/2021/10/numpy/:5:0","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"4.1 变形reshape和flatten/ravel #arr.reshape()可以将数组重塑到指定的维度： arr1 = np.arange(24) arr2 = arr1.reshape(-1,6) arr2 array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) #arr.flatten()和arr.ravel都是展平 arr3 = arr2.flatten() arr3 array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) #ravel() 在「行主序」打平时没有复制原数组，只在「列主序」打平时复制了原数组 #flatten() 在所有情况下打平时都复制了原数组 ","date":"2021-10-25","objectID":"/2021/10/numpy/:5:1","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"4.2 拆并 在 concatenate() 函数里通过设定轴，来对若干个数组按轴来合并。 例如np.concatenate([arr1, arr2], axis=0)或者np.concatenate([arr1, arr2], axis=1) vstack()：v 代表 vertical，沿着竖直方向合并，即沿着「轴 0」合并，和 concatenate(axis=0) 等价。 hstack()：h 代表 horizontal，沿着水平方向合并，即沿着「轴 1」合并，和 concatenate(axis=1) 等价。 dstack()：d 代表 depth-wise，沿着深度方向合并，即沿着「轴 2」合并，和 concatenate(axis=2) 等价。 可以在 split() 函数里通过设定轴，来对数组按轴来分裂。对二维数组， axis = 0：按行分裂，或沿竖直方向分裂 axis = 1：按列分裂，或沿水平方向分裂 vsplit()：v 代表 vertical，沿着竖直方向分裂，即沿着「轴 0」分裂，和 split(axis=0) 等价。 hsplit()：h 代表 horizontal，沿着水平方向分裂，即沿着「轴 1」分裂，和 split(axis=1) 等价。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:5:2","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"5. 数组的排序、添删 ","date":"2021-10-25","objectID":"/2021/10/numpy/:6:0","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"5.1 数组的排序 arr = np.array([5,3,2,6,1,4]) print( '排序前 arr =', arr ) arr.sort() print( '排序后 arr =', arr ) #若需要降序，则 arr[::-1] arr.sort() #原地排序 np.sort( arr ) #重新复制一份排序，不会改变 arr #间接排序 score = np.array([100, 60, 99, 80, 91]) idx = score.argsort() print( idx ) #这个 idx = [1 3 4 2 0] 存储着排序之后元素在原数组中的位置索引。 #排序result [60 80 91 99 100] ","date":"2021-10-25","objectID":"/2021/10/numpy/:6:1","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"5.2 数组的添删 #insert, delete #在数组中添加和删除操作和列表一样 #用 insert() 函数在某个特定位置之前插入元素 #用 delete() 函数删除某些特定元素 arr = np.arange(6) print( np.insert(arr, 1, 100) ) print( np.delete(arr, [1,3]) ) #[0 100 1 2 3 4 5] #[0 2 4 5] ","date":"2021-10-25","objectID":"/2021/10/numpy/:6:2","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"6. 数组的计算 数组之间直接的计算一般都是元素之间的计算，加减乘除什么的。若是需要进行线代中相似的计算，一般可以将二维数组变为矩阵来进行。 A = np.mat(arr2d) A = np.asmatrix(arr2d) #转置 #数组用 arr2d.T 操作或 arr.tranpose() 函数，而矩阵用 A.T 操作。 #求逆 #数组用 np.linalg.inv() 函数，而矩阵用 A.I 和 A**-1 操作。 # 如要使数组上相乘符合数学上的定义，用三种写法： 1. 用 @ 算术运算符 2. 用 matmul() 函数 3. 用 dot() 函数 #点乘 np.dot(向量, 向量) ","date":"2021-10-25","objectID":"/2021/10/numpy/:7:0","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Python"],"content":"广播机制 当对两个形状不同的数组按元素操作时，可能会触发广播机制 (broadcast)。具体做法，先适当复制元素使得这两个数组形状相同后再按元素操作。 ","date":"2021-10-25","objectID":"/2021/10/numpy/:7:1","tags":["Python","Numpy"],"title":"【Python】Python-Numpy","uri":"/2021/10/numpy/"},{"categories":["Linux学习"],"content":"tmux 是一款终端复用命令行工具，一般用于Terminal 的窗口管理。","date":"2021-10-21","objectID":"/2021/10/tmux/","tags":["Linux","tmux"],"title":"【Linux学习】Linux-tmux","uri":"/2021/10/tmux/"},{"categories":["Linux学习"],"content":"1.安装方式 Ubuntu下 sudo apt-get install tmux ","date":"2021-10-21","objectID":"/2021/10/tmux/:0:1","tags":["Linux","tmux"],"title":"【Linux学习】Linux-tmux","uri":"/2021/10/tmux/"},{"categories":["Linux学习"],"content":"2.大致功能 tmux（Terminal MultipleXer，意思是“终端复用器”） 第一个功能在于能实现分屏的效果。也就是说，通过一个窗口操作多个会话 远程连接时，断网后不会影响tmux内部的继续执行 ","date":"2021-10-21","objectID":"/2021/10/tmux/:0:2","tags":["Linux","tmux"],"title":"【Linux学习】Linux-tmux","uri":"/2021/10/tmux/"},{"categories":["Linux学习"],"content":"3.开始使用 3.1 新建会话 在命令行中直接输入tmux回车就可以新建一个会话了。 或者一开始就讲究一点，直接取个名字： tmux new -s sec1 3.2 tmux的结构体系 tmux的结构有点像章节结构。最高层是session会话，然后是windows窗口，最小的单位叫panes窗格。可以说，一个会话里可以有多个窗口，一个窗口里可以有多个窗格。会话实质是伪终端的集合，每个窗格表示一个伪终端，多个窗格展现在一个屏幕上，这一屏幕就叫窗口。 在每一个窗格里基本都是能实现原本正常终端能实现的功能。 3.3 基本命令和快捷键 基本操作，主要就是围绕着对会话，窗口，窗格的管理所展开的 tmux是需要组合键前缀（prefix）进行使用的，比如说要执行什么东西，得先按个ctrl+b，按完后的几秒内再按相应的快捷键。 这个ctrl+b可以通过其他方式更改为其他快捷键，比如ctrl+a 会话管理　常用命令 tmux new　创建默认名称的会话（在tmux命令模式使用new命令可实现同样的功能） tmux new -s mysession　创建名为mysession的会话 tmux ls　显示会话列表 tmux a　连接上一个会话 tmux a -t mysession　连接指定会话 tmux rename -t s1 s2　重命名会话s1为s2 tmux kill-session　关闭上次打开的会话 tmux kill-session -t s1　关闭会话s1 tmux kill-session -a -t s1　关闭除s1外的所有会话 tmux kill-server　关闭所有会话 常用快捷键 prefix s　列出会话，可进行切换 prefix $　重命名会话 prefix d　分离当前会话 prefix D　分离指定会话 窗口管理 prefix c　创建一个新窗口 prefix ,　重命名当前窗口 prefix w　列出所有窗口，可进行切换 prefix n　进入下一个窗口 prefix p　进入上一个窗口 prefix l　进入之前操作的窗口 prefix 0~9　选择编号0~9对应的窗口 prefix .　修改当前窗口索引编号 prefix ‘　切换至指定编号（可大于9）的窗口 prefix f　根据显示的内容搜索窗格 prefix \u0026　关闭当前窗口 　窗格管理 prefix %　水平方向创建窗格 prefix “　垂直方向创建窗格 prefix Up|Down|Left|Right　根据箭头方向切换窗格 prefix q　显示窗格编号 prefix o　顺时针切换窗格 prefix }　与下一个窗格交换位置 prefix {　与上一个窗格交换位置 prefix x　关闭当前窗格 prefix space(空格键)　重新排列当前窗口下的所有窗格 prefix !　将当前窗格置于新窗口 prefix Ctrl+o　逆时针旋转当前窗口的窗格 prefix t　在当前窗格显示时间 prefix z　放大当前窗格(再次按下将还原) prefix i　显示当前窗格信息 ","date":"2021-10-21","objectID":"/2021/10/tmux/:0:3","tags":["Linux","tmux"],"title":"【Linux学习】Linux-tmux","uri":"/2021/10/tmux/"},{"categories":["Linux学习"],"content":"4.一些配置 4.1 启动鼠标功能 网上好多教材都不能实现的，找了半天找到个可以用的，好像和tmux版本有关系，操作方式如下： 先按Ctrl + B， 松开以后，输入冒号，输入set -g mouse on 回车（set后有空格） 4.2 改变前缀键prefix 为 ctrl+a 快捷键可以自定义，比如将前缀改为Ctrl+a，但需要保留shell默认的Ctrl+a快捷键，按如下所示修改~/.tmux.conf文件： 1 set-option -g prefix C-a 2 unbind-key C-b 3 bind-key C-a send-prefix 4 bind-key R source-file ~/.tmux.conf \\; display-message \"~/.tmux.conf reloaded.\" 现在已将原先的Ctrl+a用prefix Ctrl+a取代，即需要按两次Ctrl+a生效。 第4行的作用是使用prefix r重新加载配置文件，并输出提示，否则需要关闭会话后配置文件才能生效，也可手动加载配置文件，在tmux终端输入”prefix :\"进入命令模式，用source-file命令加载配置文件。 注意，将多个命令写在一起作为命令序列时，命令之间要用空格和分号分隔。 ","date":"2021-10-21","objectID":"/2021/10/tmux/:0:4","tags":["Linux","tmux"],"title":"【Linux学习】Linux-tmux","uri":"/2021/10/tmux/"},{"categories":["论文笔记"],"content":"写在前面 最近一篇文章《CV圈杀疯了！继谷歌后，清华、牛津等学者又发表三篇MLP相关论文。。。》到处转发，也有很多人第一时间写了解析，本来还在研究transfomer的我也懵了。这边还没搞清楚呢，又来新的~~于是也不免俗的追一下热点吧，简单了写了一下四篇初代MLP的笔记，也就是在上面文章中出现的四篇。 1.MLP-Mixer: An all-MLP Architecture for Vision 2.Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks 3.RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition 4.Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet 回味一下多层感知机 MLP(multi-layer perceptrons)，中文就是多层感知机。刚才是接触神经网络的时候，好像就是最先接触的这个东西。就是如下图所示的这么一个东西。有输入层，隐藏层，输出层什么的，最后给个预测结果。如果学习过机器学习的一些基础课程，应该都接触过这么个东西。其实也又称之为多层全连接神经网络。也就是大量的矩阵运算balabala~~ ","date":"2021-05-17","objectID":"/2021/05/mlp/:1:0","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"1、MLP-Mixer 原文：MLP-Mixer: An all-MLP Architecture for Vision（arXiv:2105.01601） 论文链接: https://arxiv.org/abs/2105.01601 pytorch复现代码:https://github.com/d-li14/mlp-mixer.pytorch 好像说是谷歌是VIT团队的论文。还特意去对比了一下作者，果然好几个人都是一样的。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:2:0","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"1.1 模型的输入，分patch 模型的整体框架如上所示，乍一看，还以为是transformer，输入都是分patch后作为输入，和transformer图很像了。当然了，后续的操作肯定还是不一样的。简单解释一下，比如一张$256 \\times 256$的图，patch的大小是$32\\times 32$。那么他就可以得到64个patch，每个patch的参数维度为$32\\times 32\\times 3 = 3072$，也就是说这样的输入维度为 $$ \\mathbb{R}^{b \\times 64 \\times 3072} $$ 其中，b是batchsize。输入的形式和VIT应该是一致的。 顺带提一下。对于图像的低级任务，像去雾去噪超分辨等，好像最早就有说分patch去做的，单patch的超分辨去噪似乎也没太大影响，所以早期应该是有方法ptach在MLP处理的方法吧（其实我也不确定）。至于比较高级的涉及语义的任务，分类，分割什么的就不知道有没有了~~ ","date":"2021-05-17","objectID":"/2021/05/mlp/:2:1","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"1.2 模型框架，MLP layers 从input进入，首先经历一个per-patch fully-connected，其实也就是个embedding的操作。然后进入一个N次的Mixer Layer。最后池化+全连接，得到后续的分类结果 文章中的MLP layers区分成了两种，channel-mixing MLPs and token-mixing MLPs 图中，紫色框框就是token-mixing MLPs，绿色框框是channel-mixing MLPs。token-mix无非就是使得不同像素间有通信，channel-mix就是使得不同的channel间有通信。作者也提到了channel-mixing MLPs类似于1x1的卷积，token-mixing MLPs类似于卷积核为n的卷积。 在MLP layers之中，先进行一次token-mixing MLP，再进行一次channel-mixing MLP 。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:2:2","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"1.3 代码 直接看看代码吧，官方的代码是JAX/Flax框架的。 在guthub上找了一个pytorch复现的版本：https://github.com/d-li14/mlp-mixer.pytorch import torch import torch.nn as nn class MlpBlock(nn.Module): def __init__(self, hidden_dim, mlp_dim): super(MlpBlock, self).__init__() self.mlp = nn.Sequential( nn.Linear(hidden_dim, mlp_dim), nn.GELU(), nn.Linear(mlp_dim, hidden_dim) ) def forward(self, x): return self.mlp(x) ##每一次的全连接中间有个隐藏层，经过隐藏层再回到与输入一致的维度大小。 class MixerBlock(nn.Module): def __init__(self, num_tokens, hidden_dim, tokens_mlp_dim, channels_mlp_dim): super(MixerBlock, self).__init__() self.ln_token = nn.LayerNorm(hidden_dim) self.token_mix = MlpBlock(num_tokens, tokens_mlp_dim) self.ln_channel = nn.LayerNorm(hidden_dim) self.channel_mix = MlpBlock(hidden_dim, channels_mlp_dim) def forward(self, x): out = self.ln_token(x).transpose(1, 2) x = x + self.token_mix(out).transpose(1, 2) ##先进行一次token-mixing MLP out = self.ln_channel(x) x = x + self.channel_mix(out) ##再进行一次channel-mixing MLP return x class MlpMixer(nn.Module): ##描述整体的MLP-Mixer框架 def __init__(self, num_classes, num_blocks, patch_size, hidden_dim, tokens_mlp_dim, channels_mlp_dim, image_size=224): super(MlpMixer, self).__init__() num_tokens = (image_size // patch_size)**2 self.patch_emb = nn.Conv2d(3, hidden_dim, kernel_size=patch_size, stride=patch_size, bias=False) self.mlp = nn.Sequential(*[MixerBlock(num_tokens, hidden_dim, tokens_mlp_dim, channels_mlp_dim) for _ in range(num_blocks)]) self.ln = nn.LayerNorm(hidden_dim) self.fc = nn.Linear(hidden_dim, num_classes) def forward(self, x): x = self.patch_emb(x) x = x.flatten(2).transpose(1, 2) ##制造生成分patch的input x = self.mlp(x) #进行n次mlp layer x = self.ln(x) x = x.mean(dim=1) #average pooling的操作 x = self.fc(x) #全连接至分类数 return x def mixer_s32(num_classes=1000, **kwargs): return MlpMixer(num_classes, 8, 32, 512, 256, 2048, **kwargs) ","date":"2021-05-17","objectID":"/2021/05/mlp/:2:3","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"2、External Attention 原文：Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks（arXiv:2105.02358） 论文链接: https://arxiv.org/abs/2105.02358 官方Jittor和torch代码:https://github.com/MenghaoGuo/-EANet 清华计图团队的工作，所以代码也是有一份jittor框架的代码。 这篇文章提出了一种新型的attention机制，可能是因为这种attention机制的线性性质，所以会在MLP这个板块中出现。感觉还是蛮有意思的。思想也不复杂。用文章摘要的一句话就是： This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, and shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers 两个外部的、小的、可学习的和共享的存储器，存储器？是个啥？ ","date":"2021-05-17","objectID":"/2021/05/mlp/:3:0","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"2.1 self-attentio的缺陷 从摘要到introduction，作者一直都有说self-attention的缺陷在于：1.计算量大，2.他只局限于当前样本，没有关注数据集中的其他样本，缺乏信息的交互。比如说，在分割任务中，在不同样本中也存在着同一类别的特征，其他样本的特征，能否辅助该样本的分割呢。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:3:1","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"2.2 self-attention vs External Attention 文章通过对比self-attention来引出external attention。 对于上图a就是经典的self-attention。通过先将feature map投影到QKV，通过Q,K计算注意力权重，最后分配到V上： $$ \\begin{aligned} A = \\alpha_{i, j}\u0026=\\operatorname{softmax}\\left(Q K^{T}\\right) \\\\ F_{\\text {out }} \u0026=A V \\end{aligned} $$ b展示的是a的简化版，即无需投影，所有操作对feature map直接操作： $$ \\begin{aligned} A \u0026=\\operatorname{softmax}\\left(F F^{T}\\right) \\\\ F_{\\text {out }} \u0026=A F \\end{aligned} $$ c展示也就是文章所提出的external attention，文章中给出的公式是： $$ \\begin{aligned} A \u0026=\\operatorname{Norm}\\left(F M_{k}^{T}\\right) \\\\ F_{\\text {out }} \u0026=A M_{v} \\end{aligned} $$ 其中这个A表示了注意力图。$M_{k}$ 和 $M_{v}$是两个不同的记忆单元。$M \\in \\mathbb{R}^{S \\times d}$ 文章也给出了一个EA操作用于语义分割的框架图，可以看出，在backbone之后引入该模块。EA操作的前后还是给出一个shortcut，类似resnet，使用了sumation将前后直接相加起来。这个后面带个MLP操作不知道是个什么意思，文章里貌似也没看到什么说法。代码中，好像也没个，最后也就是卷积加个上采样的操作。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:3:2","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"2.3 double-normalization EA操作中，norm操作使用的是double-normalization。文章中说是为了避免输入特征的比例敏感，相当于做的两次norm。 $$ \\begin{aligned} (\\tilde{\\alpha})_{i, j} \u0026=F M_{k}^{T} \\\\\\\\ \\alpha_{i, j} \u0026=\\frac{\\exp \\left(\\tilde{\\alpha}_{i, j}\\right)}{\\sum_{k} \\exp \\left(\\tilde{\\alpha}_{k, j}\\right)} \\\\\\\\ \\alpha_{i, j} \u0026=\\frac{\\alpha_{i, j}}{\\sum_{k} \\alpha_{i, k}} \\end{aligned} $$ ","date":"2021-05-17","objectID":"/2021/05/mlp/:3:3","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"2.4 实验 文章的实验做的很多，包括图像分类、语义分割、图像生成、点云分类和点云分割等任务，应该还是想验证该注意力对高级任务的有效性，就像前面提到的分割的例子一样。EA操作，又节省复杂度，又能使得sample之间也获得关联。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:3:4","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"2.5 代码 官方Jittor和torch代码:https://github.com/MenghaoGuo/-EANet External_attention的代码： class External_attention(nn.Module): ''' Arguments: c (int): The input and output channel number. ''' def __init__(self, c): super(External_attention, self).__init__() self.conv1 = nn.Conv2d(c, c, 1) self.k = 64 self.linear_0 = nn.Conv1d(c, self.k, 1, bias=False) self.linear_1 = nn.Conv1d(self.k, c, 1, bias=False) self.linear_1.weight.data = self.linear_0.weight.data.permute(1, 0, 2) self.conv2 = nn.Sequential( nn.Conv2d(c, c, 1, bias=False), norm_layer(c)) for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.Conv1d): n = m.kernel_size[0] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, _BatchNorm): m.weight.data.fill_(1) if m.bias is not None: m.bias.data.zero_() def forward(self, x): idn = x ##刚进EA模块的时候是一个正常的四维的特征图。比如1*512*16*16 x = self.conv1(x) b, c, h, w = x.size() ## 1 512 16 16 n = h*w ## n=16*16=256 x = x.view(b, c, h*w) # b * c * n 1*512*256 attn = self.linear_0(x) # b, k, n 用kernel为1的1d卷积，得到 1*64*256 #实际上这一步的1x1卷积就是将输入乘上一个(512x64)的可学习的卷积。 attn = F.softmax(attn, dim=-1) # b, k, n attn = attn / (1e-9 + attn.sum(dim=1, keepdim=True)) # # b, k, n ##进行两次norm的操作，即double-normalization x = self.linear_1(attn) # b, c, n #再进行第二次的矩阵乘法 从1*64*256重新变回1*512*256 x = x.view(b, c, h, w)#变回四维 1*512*16*16 x = self.conv2(x) #卷积 1*512*16*16 -\u003e 1*512*16*16 x = x + idn #与输入直接值相加 1*512*16*16 -\u003e 1*512*16*16 x = F.relu(x) return x ","date":"2021-05-17","objectID":"/2021/05/mlp/:3:5","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"3、RepMLP 原文：RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition（arXiv:2105.02358） 论文链接: https://arxiv.org/abs/2105.01883 官方pytorch代码:https://github.com/DingXiaoH/RepMLP 清华丁霄汉的工作，最早关注他的工作是ICCV2019的ACNet(1908.03930)（用三个不同的卷积核的三通路卷积来训练，用一个3X3卷积核在测试）。他后续的工作也是主要是在结构重参数化。今年的CVPR2021中，也 有两篇相关的工作，分别是RepVGG和DiverseBranchBlock。对结构重参数化感兴趣的，可以关注一下这个大佬。 结构重参数化，简而言之。就是结构（或者说是模型）A对应一组参数X，结构B对应一组参数Y，如果我们能将X等价转换为Y，就能将结构A等价转换为B。在实际的使用中，使用一个较为复杂的网络进行训练，再想办法将这个复杂的网络简化，最终得到的效果是一致的。也就是说，对于同样一个输入，这个复杂的网络和简化的网络能得到完全一致的输出。 这篇RepMLP也是一篇相关的工作，正好MLP是这突如其来的热点，他就是其中之一。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:4:0","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"3.1 卷积和MLP处理图像的优缺点 这篇文章很巧妙的从深度学习处理图像中几个重要的性质开始说起。即长距离建模（或者说是全局信息），位置信息以及局部先验。 局部先验：由于conv层只处理局部邻域，图像的局部性(即一个像素与其邻居的关系比远处的像素更密切)使得卷积神经网络在图像识别中取得了成功。这是全连接操作不会有的，因为全连接操作是全局的，每个点之间都能产生关系，就不存在什么局部的说法。这也说明了卷积操作的合理性和必要性。 The locality of images (i.e., a pixel is more related to its neighbors than the distant pixels) makes Convolutional Neural Network (ConvNet) successful in image recognition, as a conv layer only processes a local neighborhood. In this paper, we refer to this inductive bias as the local prior. 全局信息：传统的纯卷积网络当中，我们会通过不断的卷积或者encoder减小尺寸，扩大感受野来获得全局信息。事实上在transformer的工作上就是一个有丰富全局信息的方法。同样的，对比卷积来说，他缺乏局部先验，所以可能需要大量的数据来进行预训练。同样的，全连接操作是天然获取全局信息的。 位置信息：很多图像是有位置先验的（比如说一个人的面部，眼睛肯定是在鼻子上面的），但是卷积操作是无法利用这些位置信息的。而全连接也是天然有位置信息的，他的数据分布是排序的。 综上所述，全连接天然拥有全局信息和位置信息，但是我也想要有局部先验呀。怎么办，如果是我的话，那就直接并行两条支路呗。但是，作者的野心不止于此，他想要一个纯的MLP网络。于是就是作者的老本行了，将卷积操作重参数化为MLP。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:4:1","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"3.2 RepMLP模块 在RepVGG中，训练时是多分支结构，推理时是单分支结构。 那在RepMLP中呢? 看上图，还是颇为复杂的。左边的这个训练阶段简单拆分一下，分为三部分Global Perceptron、Partition Perceptron、Local Perceptron。 Global Perceptron 顾名思义，Global Perceptron就是用于提取全局信息的模块。输入进入之后，分成了两条线。蓝线的操作就是一个reshape感觉，本质上就是将图片分patch，然后叠加在bathsize上面。但是直接这么做似乎缺乏了patch之间的相关性。于是就有了绿色的这条支路，将原图进行pooling，使得大小为patch的数量，也就是一个patch对应1个点。然后进行一系列操作。最后叠加到蓝线的输出中。 Partition Perceptron 从global出来之后，通过各种reshape的操作，将图像的特征图形式的样式序列化。在进行全连接的操作。因为全连接可以天然的有位置信息，于是这个模块是可以获取位置信息的。 Local Perceptron 局部信息也就是前面提到的，卷积可以获得但全连接不能获得的信息。在训练模块中，依然使用的是卷积操作。对进来的特征图，分别进行卷积核为1，3，5，7的分组卷积，得到相对于的特征图，最后合并起来。并与Partition Perceptron的输出相加。 这里还是简单的介绍了一下几个模块大概的一个情况，很多细节和具体的公式还是需要在论文中去看。这里也推荐一篇讲解（https://mp.weixin.qq.com/s/FwITC1JEG1vr2Y1ePzSvuw），将数据流都解释的很详细了，所以也不多花篇幅在这个上面了。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:4:2","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"3.3 将卷积变成全连接？怎么变？ 文章定义FC kernel为$\\mathrm{W}^{(1)}(\\mathrm{Ohw}, C h w)$，卷积的conv kernel为$\\mathrm{F}(O, C, K, K)$，padding为p 我们现在想要将FC操作和CONV操作给相加起来。 $$ \\operatorname{MMUL}\\left(\\mathrm{M}^{(\\text {in })}, \\mathrm{W}^{\\prime}\\right)=\\operatorname{MMUL}\\left(\\mathrm{M}^{(\\mathrm{in})}, \\mathrm{W}^{(1)}\\right)+\\mathrm{CONV}\\left(\\mathrm{M}^{(\\mathrm{in})}, \\mathrm{F}, p\\right) $$ 希望获得这么一个结果。即，一次fn操作和一次conv操作可以用一个fn操作就给它代替掉了。 我们知道，两个相同尺度的全连接操作是可以直接相加的，如果想要全连接和卷积能够相加的话，就要把卷积操作，转换成一个等价的全连接操作。也就是希望找到一个W满足下面这样的关系。也就是把卷积核的权重参数，想办法变成等价的全连接的权重参数（这是一定存在的，因为卷积本身就是一种特殊的全连接） $$ \\operatorname{MMUL}\\left(\\mathrm{M}^{(\\mathrm{in})}, \\mathrm{W}^{(\\mathrm{F}, p)}\\right)=\\mathrm{CONV}\\left(\\mathrm{M}^{(\\mathrm{in})}, \\mathrm{F}, p\\right) $$ 对于一个输入$M^{(in)}$,他要得到输出$M^{(out)}$。可以通过卷积实现，也可以通过全连接实现。如下式： $$ \\mathrm{M}^{(\\text {out })}=\\operatorname{CONV}\\left(\\mathrm{M}^{(\\text {in })}, \\mathrm{F}, p\\right)=\\operatorname{MMUL}\\left(\\mathrm{M}^{(\\mathrm{in})}, \\mathrm{W}^{(\\mathrm{F}, p)}\\right) $$ 下面正式看看这么做： 首先定义一下全连接操作，其实也就是矩阵乘法 $$ \\mathrm{V}^{(\\text {out })}=\\operatorname{MMUL}\\left(\\mathrm{V}^{(\\text {in })}, \\mathrm{W}\\right)=\\mathrm{V}^{(\\mathrm{in})} \\cdot \\mathrm{W}^{\\top} $$ 结合前面给的定义，也就是说我现在要寻找一个$\\mathrm{W}^{(\\mathrm{F}, p) \\top}$,输入乘上这样一个矩阵，它能起到的作用等同于F为卷积核，padding p的卷积操作。下面这个$\\mathrm{W}^{(\\mathrm{F}, p) \\top}$是我们要去寻找到哦。为了方便理解，这里也判断一下各个张量的维度，${V}^{(\\text {out })} \\in (Ohw)$，${V}^{(\\text {in })} \\in (Chw)$，$\\mathrm{W}^{(\\mathrm{F}, p) \\top} \\in (Chw,Ohw)$ $$ \\mathrm{V}^{(\\text {out })}=\\mathrm{V}^{(\\mathrm{in})} \\cdot \\mathrm{W}^{(\\mathrm{F}, p) \\top} $$ 先插入一个单位矩阵$I(Chw,Chw)$，不影响运算，值和维度都不会发生改变： $$ \\mathrm{V}^{(\\mathrm{out})}=\\mathrm{V}^{(\\mathrm{in})} \\cdot\\left(\\mathrm{I} \\cdot \\mathrm{W}^{(\\mathrm{F}, p) \\mathrm{T}}\\right) $$ $\\mathrm{W}^{(\\mathrm{F}, p) \\top}$终究还是需要源自于卷积核F的（不管他是怎么变过来的），上面$(\\mathrm{I} \\cdot \\mathrm{W}^{(\\mathrm{F}, p) \\mathrm{T}})$这个式子可以表示的是对$\\mathrm{M}^{(\\mathrm{I})}=reshape(I)$进行F卷积操作。$\\mathrm{M}^{(\\mathrm{I})} \\in (C h w, C, h, w)$,$\\mathrm{F} \\in (O, C, K, K)$,$\\operatorname{CONV}\\left(\\mathrm{M}^{(\\mathrm{I})}, \\mathrm{F}, p\\right)$的结果的维度应该是$(Chw,O,h,w)$，再经过下面第三个公式的RS后有变成了二维的$(Chw,Ohw)$，再让输入乘以它。得到的结果刚刚好是输出想有的尺寸$(Ohw)$ $$ \\begin{array}{c} \\mathrm{M}^{(\\mathrm{I})}=\\operatorname{RS}(\\mathrm{I},(C h w, C, h, w)) \\\\ \\mathrm{I} \\cdot \\mathrm{W}^{(\\mathrm{F}, p) \\top}=\\operatorname{CONV}\\left(\\mathrm{M}^{(\\mathrm{I})}, \\mathrm{F}, p\\right) \\\\ \\mathrm{V}^{(\\mathrm{out})}=\\mathrm{V}^{(\\mathrm{in})} \\cdot \\mathrm{RS}\\left(\\mathrm{I} \\cdot \\mathrm{W}^{(\\mathrm{F}, p) \\top},(C h w, O h w)\\right) \\end{array} $$ 第一个公式意思就是将矩阵I变成4维的形式，第二个公式表示的是对I乘以这个矩阵需要等同于对的I的reshape做卷积操作。第三个式子表示，将卷积得到的重新reshape成矩阵，并让输入乘以他。 前面搞了这么久，绕过来绕过去。但是我们要明确的还是，我们已知的是卷积，要求的是FC的权重剧中。结合前面的分析可以得到： $$ \\mathrm{W}^{(\\mathrm{F}, p)}=\\mathrm{RS}\\left(\\mathrm{CONV}\\left(\\mathrm{M}^{(\\mathrm{I})}, \\mathrm{F}, p\\right),(C h w, O h w)\\right)^{\\top} $$ 文章最后也给了伪代码以供参考： Algorithm 1 PyTorch code for converting groupwsie conv into FC. Input: C, h, w, g, O, conv kernel, conv bias I = torch.eye(C * h * w // g).repeat(1, g).reshape(C * h * w // g, C, h, w) fc kernel = F.conv2d(I, conv kernel, padding=conv kernel.size(2)//2, groups=g) fc kernel = fc kernel.reshape(O * h * w // g, C * h * w).t() # Note the transpose fc bias = conv bias.repeat interleave(h * w) return: fc kernel, fc bias ","date":"2021-05-17","objectID":"/2021/05/mlp/:4:3","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"3.4 总结和思考 其实，我想了很久没想明白。后来，也有一点点自己的理解吧。首先明确任务，就是要将一个卷积核参数转化成全连接权重参数，他们的维度不一样。卷积是一种特殊的全连接，或者说是稀疏的全连接。如果现在拿到手一个卷积操作，不管通过手工还好，拼凑还好，肯定能变换成一个合适的全连接权重，是满足参数要求的。 那么作者妙在哪里呢？他想了一个办法，使得这个过程能够很轻松的通过普通运算就能搞出来。这个转换有效，过程可微，满足了我们去训练网络非常重要的一些性质。 那么是方法是什么呢？其实前面很多公式已经给出了具体过程了。其实，在我理解看来，就是利用了单位矩阵的良好性质，文章中称之为identity matrix（自身的矩阵）。在线性代数中，和单位矩阵进行乘法运算都会成为自己本身。同样的，在这里是不是可以理解为，对单位矩阵进行卷积，相当于是在保留这个卷积的运算，后续的reshape成二维也好，怎么样也好，他保存的计算属性应该是一致的，所以可以去等价的替换。 不管是ACNet还是RepVgg，作者都是对卷积操作做替换。这次直接卷积到全连接。其实思路可能是好想的，因为卷积源于全连接，他们之间有着千丝万缕的联系。但是难就难在如何巧妙的转换，如何让这个过程可微又方便，所以真的很妙~ ","date":"2021-05-17","objectID":"/2021/05/mlp/:4:4","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"参考 1.https://mp.weixin.qq.com/s/FwITC1JEG1vr2Y1ePzSvuw 2.https://zhuanlan.zhihu.com/p/369970953 ","date":"2021-05-17","objectID":"/2021/05/mlp/:4:5","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"4、Do You Even Need Attention? 原文：Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet（arXiv:2105.02723） 论文链接: https://arxiv.org/abs/2105.02723 pytorch代码:https://github.com/lukemelas/do-you-even-need-attention 牛津大学的工作（居然只有一个作者），这是一篇很短的文章，仅仅只有三页。这篇短文更像一篇实验报告，可能是想让狂热的transformer稍微冷静一下下吧。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:5:0","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"4.1 将attention层换成了feed-forward层 文章开篇质疑了attention的作用，认为VIT起作用不一定是因为attention。作者验证的方式就是将vit中的attention层换成了feed-forward层进行实验。 实验结果表明VIT的强大性能可能更多地归因于其他因素，而不是注意力机制，例如由patch embedding和训练策略产生的影响。 ","date":"2021-05-17","objectID":"/2021/05/mlp/:5:1","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["论文笔记"],"content":"4.2 具体的代码实现 from torch import nn class LinearBlock(nn.Module): def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act=nn.GELU, norm=nn.LayerNorm, n_tokens=197): # 197 = 16**2 + 1 super().__init__() self.drop_path = DropPath(drop_path) if drop_path \u003e 0. else nn.Identity() # FF over features self.mlp1 = Mlp(in_features=dim, hidden_features=int(dim*mlp_ratio), act=act, drop=drop) self.norm1 = norm(dim) # FF over patches self.mlp2 = Mlp(in_features=n_tokens, hidden_features=int(n_tokens*mlp_ratio), act=act, drop=drop) self.norm2 = norm(n_tokens) def forward(self, x): x = x + self.drop_path(self.mlp1(self.norm1(x))) x = x.transpose(-2, -1) x = x + self.drop_path(self.mlp2(self.norm2(x))) x = x.transpose(-2, -1) return x class Mlp(nn.Module): def __init__(self, in_features, hidden_features, act_layer=nn.GELU, drop=0.): super().__init__() self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() self.fc2 = nn.Linear(hidden_features, in_features) self.drop = nn.Dropout(drop) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x 代码也比较好看， 就是相比vit换attention为MLP ","date":"2021-05-17","objectID":"/2021/05/mlp/:5:2","tags":["MLP","结构重参数化"],"title":"【论文笔记】MLP杀疯了？四篇初代MLP论文笔记","uri":"/2021/05/mlp/"},{"categories":["Linux学习"],"content":"git是用于Linux内核开发的版本控制工具。","date":"2021-05-16","objectID":"/2021/05/git/","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"写在前面 偶尔也会使用git，不过大部分就是执行一下git clone，上传blog也就是照着网上的命令打一打，出现什么错误了也不知道什么意思。对git的运行原理，如何版本控制，以及对git与github之间的操作不是很了解。 下定决心，花了点时间，学习了一个教程，记了一点点小笔记，以供自查。 教程：廖雪峰的Git教程 ","date":"2021-05-16","objectID":"/2021/05/git/:1:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"一、创建版本库 1.1 刚开始设置名字邮箱 安装完成后设置一个全局量，说明这个机子上的仓库都是这个人的。 $ git config --global user.name \"Your Name\" $ git config --global user.email \"email@example.com\" 1.2 在需要git管理的目录下使用git init命令把这个目录变成可以管理的仓库。 $ git init #文件夹中会出现一个.git的文件夹，用来管理仓库 1.3 新建一个文件，或者修改一个文件后 用命令git add告诉Git，把文件添加到仓库： $ git add readme.md #这个文件需要添加到仓库 用命令git commit告诉Git，把文件提交到仓库： $ git commit -m \"wrote a readme file\" ##-m 命令后面写上本次操作做了啥，用于后续查看 个人感觉：在写代码的过程中，git add命令可以多次一直用，git commit最好在实现一个功能什么的之后再使用，不然会形成居多的版本，后期也不利于版本管理。 至此，已经创建好一个仓库了。 ","date":"2021-05-16","objectID":"/2021/05/git/:2:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"二、历史版本回溯 2.1 使用git status命令掌握仓库当前的状态 比如说进行修改后没有add，就会提醒你还没推到仓库。 add后再git status commit后再git status 可以看出三步情况下status不同的状态。 2.2 可以使用git diff查看difference，修改了什么内容 这里就可以看到红色行是原来的，绿色行是改后的 2.3 使用git log命令显示从最近到最远的提交日志(commmit次数) 使用后，可以看到每次的提交时间，以及加的备注。 这里进行了两次改变，共有三个版本。其中，最上面的是当前版本，git中一般用HEAD来表示。上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本可以写成HEAD~100 2.4 使用git reset命令退回到上一个版本 $ git reset --hard HEAD^ HEAD is now at e475afc add distributed git reset命令的本质是一个，Git在内部有个指向当前版本的HEAD指针，这个指针往不同的地方指。 使用了git reset命令之后，再使用git log命令会发现原来的不见了。可以通过commit代码号找回来 $ git reset --hard 7a09ed HEAD is now at 83b0afe append GPL 如果找不到了，可以使用git reflog命令，git reflog用来记录你的每一次命令，可以找到执行某一次commit的commit号 2.5 工作区和暂存区 目前文件所放置的区域就是工作区，工作区有一个隐藏目录.git，里面装的是Git的版本库。 Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 用git add把文件添加进去，实际上就是把文件修改添加到暂存区； 用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支（如果没有新建分支，那默认的分支就是git给我们默认创建的master分支）。 当不明确，目前的版本是否已经推送到分支，可以使用git diff HEAD -- readme.txt命令可以查看工作区和版本库里面最新版本的区别 2.6 撤销修改 想要撤销修改存在三种情况。 已经在文件中改了，却还没有git add 已经git add到暂存区了，还没有git commit 已经git commit了 ①已经在文件中改了，却还没有git add 直接手动删了相关修改即可 或者使用git checkout -- file命令可以丢弃工作区的修改 ②已经git add到暂存区了，还没有git commit 分两步，第一步用命令git reset HEAD \u003cfile\u003e，以把暂存区的修改撤销掉（unstage），重新放回工作区。然后再进行暂存区的修改，即使用git checkout -- file命令 ③已经git commit了 直接版本回退到上一个版本，git reset --hard HEAD^ 2.7 命令git rm用于删除一个文件 如果删错了，可以使用git checkout -- test.txt把误删的文件恢复到最新版本： git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 ","date":"2021-05-16","objectID":"/2021/05/git/:3:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"三、远程仓库 可以使用git remote命令，查看远程库的情况 3.1 配置本地和远程端的传输协议 本地Git仓库和GitHub仓库之间的传输是通过SSH加密的 本地端设置 需要创建SSH Key ssh-keygen -t rsa -C \"youremail@example.com\"。之后，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 远程端设置 在github的SSH keys中添加ssh key 因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。 3.2添加到远程端 在github上创建好一个新的仓库。 在本地的learngit仓库下运行命令： $ git remote add origin git@github.com:username/learngit.git 这个username是自己的github用户名。 添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库。 下一步，就可以把本地库的所有内容推送到远程库上： $ git push -u origin master #把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 之后，只要本地做了提交，就可以通过命令，把本地master分支的最新修改推送至GitHub $ git push origin master 删除远程库 如果添加的时候地址写错了，或者就是想删除远程库，可以用git remote rm \u003cname\u003e命令。使用前，建议先用git remote -v查看远程库信息： 然后，根据名字删除，比如删除origin： $ git remote rm origin 此处的“删除”其实是解除了本地和远程的绑定关系，并不是物理上删除了远程库。远程库本身并没有任何改动。要真正删除远程库，需要登录到GitHub，在后台页面找到删除按钮再删除。 3.3 从远程库克隆 要克隆一个仓库，首先必须知道仓库的地址，然后使用git clone命令克隆。 Git支持多种协议，包括https，但ssh协议速度最快。 ","date":"2021-05-16","objectID":"/2021/05/git/:4:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"四、分支管理 分支在实际中有什么用呢？假设你准备开发一个新功能，但是需要两周才能完成，第一周你写了50%的代码，如果立刻提交，由于代码还没写完，不完整的代码库会导致别人不能干活了。如果等代码全部写完再一次提交，又存在丢失每天进度的巨大风险。现在有了分支，就不用怕了。你创建了一个属于你自己的分支，别人看不到，还继续在原来的分支上正常工作，而你在自己的分支上干活，想提交就提交，直到开发完毕后，再一次性合并到原来的分支上，这样，既安全，又不影响别人工作。 4.1 分支的创建和合并 图的形式理解分支 master分支是主分支，事实上他是一个指针。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。 一开始的时候，master分支是一条线，Git用master指向最新的提交，再用HEAD指向master，就能确定当前分支，以及当前分支的提交点： 现在创建一个新的分支，例如dev时，也就是Git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示此时的当前分支在dev上： 接下来，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变： 之后，如果我们在dev上的工作完成了，想把dev合并到master上。最简单的方法，就是直接把master指向dev的当前提交，就完成了合并： 合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支： 分支的代码实现 可以将上面的步骤代码实现： 1.创建dev分支，然后切换到dev分支： $ git checkout -b dev或者使用$ git switch -c dev#Switched to a new branch 'dev'#git checkout命令加上-b参数表示创建并切换 可以使用git branch命令查看当前分支 2.对内容进行更改后add，commit。并切换回master主分支 如果这个时候查看文件，发现是修改之前的内容。因为是在master主分支上并没有进行过修改 3.把dev分支的工作成果合并到master分支上 $ git merge dev#git merge命令用于合并指定分支到当前分支 4.合并完成后，删除dev分支： git branch -d dev#Deleted branch dev switch命令 最新版本的Git提供了新的git switch命令来切换分支： 创建并切换到新的dev分支，可以使用： $ git switch -c dev 直接切换到已有的master分支，可以使用： $ git switch master 使用新的git switch命令，比git checkout要更容易理解。 命令总结： 查看分支：git branch 创建分支：git branch \u003cname\u003e 切换分支：git checkout \u003cname\u003e或者git switch \u003cname\u003e 创建+切换分支：git checkout -b \u003cname\u003e或者git switch -c \u003cname\u003e 合并某分支到当前分支：git merge \u003cname\u003e 删除分支：git branch -d \u003cname\u003e 4.2 分支合并中的冲突 如果出现不同的分支都进行的提交，分支都往前进了，这时候合并就可能会有冲突。（比如对同一个地方做不同的更改） 这样使用合并命令后，需要手动去修改。 用git log --graph命令可以看到分支合并图。 最后，删除子分支。 4.3 禁用Fast forward模式的分支合并 之前fast forward的分支合并是这样的: 禁用fast forward的分支合并是这样的： 使用的命令是 git merge --no-ff -m \"merge with no-ff\" dev#因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。 合并分支时，加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 4.4 利用git stash命令保存现场 暂时不想提交当前分支正在做的工作，又需要去其他分支工作。可以利用git stash命令保存现场 git stash##该分支里的内容就被藏起来了 用git stash list命令查看藏起来的内容 用git stash pop，恢复的同时把stash内容也删了 4.5 多人协作 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 ","date":"2021-05-16","objectID":"/2021/05/git/:5:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"五、标签管理 标签也是一个指针，可以理解为commit号的简化版。比如发布不同的版本时候，可以有不同的标签。 5.1创建标签 敲命令git tag \u003cname\u003e就可以打一个新标签： $ git tag v1.0 可以用命令git tag查看所有标签： $ git tagv1.0 默认标签是打在最新提交的commit上的。如果想给历史commit打标签，找到历史提交的commit id，然后打上就可以了。 比方说要对某次提交打标签，它对应的commit id是f52c633，敲入命令： $ git tag v0.9 f52c633 还可以创建带有说明的标签，用-a指定标签名，-m指定说明文字： $ git tag -a v0.1 -m \"version 0.1 released\" 1094adb 可以用git show \u003ctagname\u003e查看标签信息 5.2 操作标签 如果标签打错了，也可以删除： $ git tag -d v0.1 如果要推送某个标签到远程，使用命令git push origin \u003ctagname\u003e： 或者，一次性推送全部尚未推送到远程的本地标签git push origin --tags ","date":"2021-05-16","objectID":"/2021/05/git/:6:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"六、 Git的可配置项 6.1显示颜色 比如，让Git显示颜色，会让命令输出看起来更醒目： $ git config --global color.ui true 6.2 配置别名 如果敲git st就表示git status那就简单多了，当然这种偷懒的办法我们是极力赞成的。 我们只需要敲一行命令，告诉Git，以后st就表示status： $ git config --global alias.st status 当然还有别的命令可以简写，很多人都用co表示checkout，ci表示commit，br表示branch： $ git config --global alias.co checkout$ git config --global alias.ci commit$ git config --global alias.br branch 配置一个git last，让其显示最后一次提交信息： $ git config --global alias.last 'log -1' 甚至可以把lg配置成了： git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" 就有这样的效果 ","date":"2021-05-16","objectID":"/2021/05/git/:7:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"七、Git的GUI工具SourceTree SourceTree里可以add，commit等 ","date":"2021-05-16","objectID":"/2021/05/git/:8:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["Linux学习"],"content":"八、vscode中git的使用 因为平时写代码还是会在一些平台上写的，每次都打开git的页面版本控制肯定会很麻烦。不过这些这些软件一般都会提供版本控制的。vscode和pycharm本身也有可以支持git。可以寻找各种帖子，有详细解释。 ","date":"2021-05-16","objectID":"/2021/05/git/:9:0","tags":["git","Linux"],"title":"【Linux学习】Git学习笔记","uri":"/2021/05/git/"},{"categories":["论文笔记"],"content":"Involution: Inverting the Inherence of Convolution for Visual Recognition","date":"2021-03-14","objectID":"/2021/03/involution/","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"原文：Involution: Inverting the Inherence of Convolution for Visual Recognition 论文链接: https://arxiv.org/abs/2103.06255 pytorch official code：https://github.com/d-li14/involution 笔记时间：2020.03.14 文章最早发表在了CVRP2021 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:0","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"写在前面 这是一篇搞计算操作的文章，第一次读这方面的文章，可能有些认识有些问题。期待作者的官方解读。 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:1","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"1.简介 卷积操作有两个重要的特征（spatial-agnostic and channel-specific） 空间无关的特征虽然提高了效率但是剥夺了卷积核适应于不同空间位置的多种视觉模式的能力。另一方面，卷积内部的通道间冗余的问题在许多成功的深度神经网络中都很突出 文章提出了与卷积相对称的内卷，具有的特性是（spatial-specific and channel-agnostic） 这种操作统一了Self-Attention和卷积。 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:2","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"2.怎么理解卷积操作的两个重要特征（spatial-agnostic / channel-specific）？ 对于一个常规卷积操作来讲，从一个C1通道数的input想得到一个C2通道数的output，需要C2个卷积核。如果是3x3卷积的话，每个卷积核的尺寸是C1x3x3的。这个C1x3x3的的卷积核，我们完全可以把它看成是C1个独立的3x3的卷积核。也就是说，在生成output的每一个通道的时候，对于input的每一个通道都有一个独立的3x3的卷积核。 在input的一个通道上，整个这一个通道都是复用一个卷积核的。这就是所谓的空间无关（spatial-agnostic），我现在用什么卷积核和目前所在的空间位置没有关系，只和我现在所在的通道位置有关系。还有一种说法，这种性质叫平移不变性。 而ouput中每一个通道的形成都需要依赖于input里C1个卷积核在C1个通道中的计算。这可能就叫特定于通道（channel-specific）。文章中对此的解释是： 在Channel域中，卷积核的频谱负责收集编码在不同Channel中的不同信息，满足channel-specific特性 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:3","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"3.内卷（Involution）的提出 作者认为卷积操作的两个特征虽然也有一定的优势，但同样也有缺点。所以提出了Involution，Involution所拥有的特征正好和卷积相对称，即 spatial-specific and channel-agnostic 那就是通道无关和特定于空间。和卷积一样，内卷也有内卷核（involution kernels）。内卷核在空间范围上是不同的，但在通道之间共享。看到这里就有一定的画面感了。 内卷的优点： 1.可以在更大的空间范围中总结上下文信息，从而克服long-range interaction（本来的卷积操作只能在特定的小空间如3x3中集合空间信息） 2.内卷可以将权重自适应地分配到不同的位置，从而对空间域中信息量最大的视觉元素进行优先级排序。（本来的卷积在空间的每一个地方都是用到同一个卷积核，用的同一套权重） ","date":"2021-03-14","objectID":"/2021/03/involution/:0:4","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"4.内卷核是怎么形成的？ 与卷积核不同，内卷核是基于单个像素的而不是其与相邻像素的关系。 内卷的内卷核的形状取决于输入特征图X的尺寸。有一个kernel generation function，代表从输入生成内卷核：，即从某个像素生成到内卷核。 在这个生成公式中，可以看到两个W代表的是线性变化。W0将 1x1xC 的某一像素的表示通过线性变化降维到C/r (r为一个ration，代表减少比率)。σ代表的是BN和非线性激活。W2将1x1xC/r变化为KxKxG。这便是involution kernels。 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:5","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"5.这个内卷操作到底是怎么操作呢？ 文章中为了帮助读者理解这个内卷操作，有一张图和一段伪代码。可以结合起来理解整体的操作。 如上图所示，内卷核就是那个H，这里有个参数叫G，代表一个组中共享一个内卷核的数量。（在图中G为1） 大概有如下步骤： 1.以单个像素为条件产生内卷核 2.进行乘加运算，就是先把核拉成KxKxC，与对应的位置相乘，再把KxK个1x1xC的加起来，代替原来该像素的位置 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:6","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"6.RedNet 基于内卷的操作基础上设计了一个叫RedNet的网络，作者称可以实现优于基于卷积的ResNet和基于自我关注的图像分类模型的性能。 也就是再resnet的bottleneck的位置上替换了卷积，具体可以看论文 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:7","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"7.总结 这个内卷感觉就是想把这个核同时作用在通道和空间上。同时这个核的生成又来自于单个像素点1x1xC，提取了跨通道的信息。 网络学习的部分还是挺多的，要学从单个像素变成核的过程，要学经过核变出输出的过程。 ","date":"2021-03-14","objectID":"/2021/03/involution/:0:8","tags":["卷积"],"title":"【论文笔记】Involution","uri":"/2021/03/involution/"},{"categories":["论文笔记"],"content":"RepVGG: Making VGG-style ConvNets Great Again","date":"2021-01-19","objectID":"/2021/01/repvgg/","tags":["轻量级网络","结构重参数化"],"title":"【论文笔记】RepVGG","uri":"/2021/01/repvgg/"},{"categories":["论文笔记"],"content":"写在前面 文章还未正式发表出来，暂时是挂在了arxiv上。感觉很有意思，读这篇文章之前看了一眼作者在ICCV2019上的ACNet（Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks）（已录取cvpr2021） 感觉两篇文章的核心应该是异曲同工之妙。 从我读文章的感觉，核心点简单来看就叫 训练和推理（测试）不是一个网络 该文章的作者在知乎上也写了自己的笔记：https://zhuanlan.zhihu.com/p/344324470 文章的细节内容大家可以看作者笔记和原文，这里就记录一点自己的感受。 ","date":"2021-01-19","objectID":"/2021/01/repvgg/:1:0","tags":["轻量级网络","结构重参数化"],"title":"【论文笔记】RepVGG","uri":"/2021/01/repvgg/"},{"categories":["论文笔记"],"content":"让vgg再次伟大？ 对于一个新入门cv的新人，对vgg的感觉就是一个很老的老古董，不怎么想去看的东西。文章的标题就比较吸引我。 自vgg之后，网络加深，网络加宽，网络加注意力，NAS。各种技术层出不穷。我们不怎么关注vgg，可能还是因为vgg效果对比后来者实在太差了。差归差，但是他有自己的有优势。 所说的“VGG式”指的是： 没有任何分支结构。即通常所说的plain或feed-forward架构。 仅使用3x3卷积。 仅使用ReLU作为激活函数。 也就是说这个“VGG式”，相比我们现在所看到的网络而言非常非常的简单。除此之外，他还快（具体可见论文）。 ","date":"2021-01-19","objectID":"/2021/01/repvgg/:2:0","tags":["轻量级网络","结构重参数化"],"title":"【论文笔记】RepVGG","uri":"/2021/01/repvgg/"},{"categories":["论文笔记"],"content":"vgg虽然快，但是差怎么办？ vgg虽然快，但是效果并不好，他甚至连个分支结构都没有。于是作者就说，既然分支结构有利于训练，那我训练就用分支结构呗。既然vgg式快，那我推理（测试）的时候就用vgg呗。那么问题来了，训练的网络和推理的网络不一样？那怎么搞？ 作者说等价转换,这就是文章与其他的文章的不同了。 对于一般的方法来讲是这样的： graph LR A(训练好一个模型 M1)--\u003eB(部署M1,用M1推理/测试) 对于作者的想法是： graph LR A(训练好一个模型 M1)--\u003eB(将这个模型M1等价的变成模型M2)--\u003e C(部署M2,用模型M2推理/测试) 于是就可以同时利用了：1.训练时模型的优点（可以达到更好的收敛结果和效果）2.推理时的高速，省内存。 那么刚才的问题又来了，也是最重要的问题~训练的网络和推理的网络不一样？这怎么等价转换呢？ 其实思路也蛮简单的。可以看下图： 作者很贴心，这个图搞了两列。左边这列表示的是这个网络结构的变化的过程。右边这列表示的是卷积核参数的变化过程（当输入和输出的通道都是2）。 从左上角中可以看出作者设计的训练块，是一个多分支的并列了3x3，1x1，skip三条线的block。我们现在的目的是，要把它训练完后的参数集中到一个3x3的卷积核当中。怎么集中呢？其实想法看起来还是蛮简单的。 1x1卷积可以看成是一个特殊（卷积核中有很多0）的3x3卷积，而恒等映射是一个特殊（以单位矩阵为卷积核）的1x1卷积。于是就有了三个3x3的卷积核，把他们相加一下，那么效果就和原模型一毛一样啦。 ","date":"2021-01-19","objectID":"/2021/01/repvgg/:3:0","tags":["轻量级网络","结构重参数化"],"title":"【论文笔记】RepVGG","uri":"/2021/01/repvgg/"},{"categories":["论文笔记"],"content":"总结一下 其实作者那篇ACNet就展露出这种训练网络通过一定的方式转换成快速的推理网络的思想。而这篇文章，应该是专门为这种叫“VGG式”而设计的网络。 还有就是乍一看可能会觉得我直接训练vgg不就好了，为什么搞来搞去还是那个模型结构，却比我原来训练的结果要好呢？ 感觉吧，神经网络就是一个巨大的黑箱子，很多情况下训练可能只会陷入局部最优却找不到更优的结果。这就是个优化问题。理论上，足够大的网络可能可以找到一个很精准的拟合结果，但是就是我们优化的过程中找不到它，我们只能想办法找到尽可能离得近的结果了。 不知道是不是我孤陋寡闻了，不知道接下来会不会有一些系列的相关工作会出现。会不会能有人去解决relu的合并？如何去将这种想法运用在一些任务的特定模块当中呢？魔改出更好的结果？…… 本文可能有许多不当之处，如有不对之处还望批评指正。 ","date":"2021-01-19","objectID":"/2021/01/repvgg/:4:0","tags":["轻量级网络","结构重参数化"],"title":"【论文笔记】RepVGG","uri":"/2021/01/repvgg/"},{"categories":["论文笔记"],"content":"参考 1.Ding, Xiaohan, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. “RepVGG: Making VGG-style ConvNets Great Again.” arXiv preprint arXiv:2101.03697. 2.Ding, Xiaohan, Yuchen Guo, Guiguang Ding, and Jungong Han. “Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks.” In Proceedings of the IEEE International Conference on Computer Vision, pp. 1911-1920. 2019. 3.https://zhuanlan.zhihu.com/p/344324470 4.https://github.com/megvii-model/RepVGG ","date":"2021-01-19","objectID":"/2021/01/repvgg/:5:0","tags":["轻量级网络","结构重参数化"],"title":"【论文笔记】RepVGG","uri":"/2021/01/repvgg/"},{"categories":["论文笔记"],"content":"Squeeze-and-Excitation Network","date":"2020-12-08","objectID":"/2020/12/senet/","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"SENet 原文：Squeeze-and-Excitation Network [Cited by 4940] 论文链接: https://arxiv.org/abs/1804.03999 Caffe official code: https://github.com/hujie-frank/SENet pytorh high-star 复现：https://github.com/moskomule/senet.pytorch 笔记时间：2020.12.08 文章收录于在CVPR2018和TPAMI ","date":"2020-12-08","objectID":"/2020/12/senet/:0:0","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"写在前面 读了attention unet之后想读一下attention的经典文章，senet是做channel attention的最早的文章。引用率也很高。文章主要的工作是引入了se block，进行了通道间的注意力。文章中做了大量的消融实验，来证明网络的有效性。 ","date":"2020-12-08","objectID":"/2020/12/senet/:1:0","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"注意力机制 对注意力机制的设置大概可以理解为：使用一些网络去计算一个权重，把这个权重与feature map进行运算，对这个feature map进行改变，得到加强注意力后的feature map。 ","date":"2020-12-08","objectID":"/2020/12/senet/:1:1","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"Abstract 卷积操作可以融合空间和通道的特征，大多数的研究主要针对对空间特征的提取。本文提出的SENet主要是研究通道间关系的。其可 自适应的校正通道特征。 ","date":"2020-12-08","objectID":"/2020/12/senet/:2:0","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"Introduction 主要还是阐述了，卷积提取特征是非常重要的研究点。注意力机制可对特征进行校正，校正后的特征可保留有价值的特征，剔除没价值的特征。本文又重点关注了通道的注意力提取。 ","date":"2020-12-08","objectID":"/2020/12/senet/:3:0","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"Squeeze-and-Excitation (SE) Block 文章最重点的部分也就是SE block的提出。 之所以是叫se，是因为作者把操作分为了两步，一步叫Squeeze，一步叫Excitation ","date":"2020-12-08","objectID":"/2020/12/senet/:4:0","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"先上代码 class SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) #对应Squeeze操作 y = self.fc(y).view(b, c, 1, 1) #对应Excitation操作 return x * y.expand_as(x) ","date":"2020-12-08","objectID":"/2020/12/senet/:4:1","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"Squeeze: Global Information Embedding 直接用中文翻译的话，squeeze是挤压的意思。 上图中的画线部分就是squeeze的操作，具体来说他就是是对应一个全局平均池化的操作。将一个c通道，hxw的特征图，压成c通道1x1。于是这个得到的结果是能表示全局信息的。 The transformation output U can be interpreted as a collection of the local descriptors whose statistics are expressive for the whole image. It is proposed to squeeze global spatial information into a channel descriptor. This is achieved by using global average pooling to generate channel-wise statistics. 也就是说这个操作是为了让将全局空间信息压缩到通道描述符中。 在代码中，就是对应的自适应平均池化到1x1 self.avg_pool = nn.AdaptiveAvgPool2d(1) y = self.avg_pool(x).view(b, c) #对应Squeeze操作 ","date":"2020-12-08","objectID":"/2020/12/senet/:4:2","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"Excitation: Adaptive Recalibration 直接用中文翻译的话，excitation是激发的意思，直接理解好像不是很好理解的感觉。Adaptive Recalibration意思是自适应重新校准，好像也不是很好理解。直接看他是怎么操作的吧。 上图中的画线部分就是excitation的操作，具体来说包含两个全连接层。对squeeze全局池化后得到的结果（已经是可以看作一个C维向量），进行全连接，得到C/r维的向量，在进行Reulu激活，再对进行一次全连接，将C/r维的向量变回C维向量，再进行sigmoid激活（使得数值位于0-1之间），这便是得到了权重矩阵。 看代码可以更好的理解这个过程: self.fc = nn.Sequential( #第一次全连接，降低维度 nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), #第二次全连接，恢复维度 nn.Linear(channel // reduction, channel, bias=False), #sigmoid激活，使得值位于0-1之间 nn.Sigmoid() y = self.fc(y).view(b, c, 1, 1) #对应Excitation操作 return x * y.expand_as(x)#把权重矩阵赋予到特征图 最后的操作就是把这个权重矩阵和U进行相乘计算，把权重赋予上去。 ","date":"2020-12-08","objectID":"/2020/12/senet/:4:3","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"SE block 分开看完之后，再整合起来看就是如下图这样的操作过程。 先全局池化，再全连接将低维度，再rulu激活，再全连接恢复维度，再sigmoid激活。 ","date":"2020-12-08","objectID":"/2020/12/senet/:4:4","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"SE-Inception \u0026 SE-ResNet se块是即插即用的，可以集成到inception和resnet当中 ","date":"2020-12-08","objectID":"/2020/12/senet/:5:0","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"Ablation Study ","date":"2020-12-08","objectID":"/2020/12/senet/:6:0","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"1.Reduction ratio的选择 也就是excitation中第一个全连接降维到底降多少的问题，这是一个超参数。实验证明，在同时考虑精度和参数量的情况下选择r=16比较合适。 ","date":"2020-12-08","objectID":"/2020/12/senet/:6:1","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"2.Squeeze Operator squeeze操作中用平均池化和最大池化哪个好？平均池化好 ","date":"2020-12-08","objectID":"/2020/12/senet/:6:2","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"3.Excitation Operator excitation操作中第二个全连接后的激活层怎么选？ sigmoid ","date":"2020-12-08","objectID":"/2020/12/senet/:6:3","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"4.Different stages se块放在resnet的哪个stage好？还是全放好？ 全放好 ","date":"2020-12-08","objectID":"/2020/12/senet/:6:4","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"5.Integration strategy 再resnet中，se块怎么放比较好？有四种放的方式，第一种最好。 ","date":"2020-12-08","objectID":"/2020/12/senet/:6:5","tags":["注意力机制","通道注意力"],"title":"【论文笔记】SENet","uri":"/2020/12/senet/"},{"categories":["论文笔记"],"content":"Attention U-Net:Learning Where to Look for the Pancreas","date":"2020-12-05","objectID":"/2020/12/attunet/","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"原文：Attention U-Net:Learning Where to Look for the Pancreas [Cited by 440] 论文链接: https://arxiv.org/abs/1804.03999 pytorch official code: https://github.com/ozan-oktay/Attention-Gated-Networks 笔记时间：2020.12.5 文章收录于在MIDL(Medical Imaging with Deep Learning)‘2018 ","date":"2020-12-05","objectID":"/2020/12/attunet/:0:0","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"写在前面 方法部分感觉写的有点乱，如果有写不清楚的欢迎指出。我改~ 这篇文章应该是比较早的把软attention的思想引入到医学图像当中的。 ","date":"2020-12-05","objectID":"/2020/12/attunet/:1:0","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"注意力 注意力分为Hard Attention和 Soft Attention 硬注意力：一次选择一个图像的一个区域作为注意力，设成1，其他设为0。他是不能微分的，无法进行标准的反向传播，因此需要蒙特卡洛采样来计算各个反向传播阶段的精度。 考虑到精度取决于采样的完成程度，因此需要其他技术（例如强化学习）。 软注意力：加权图像的每个像素。 高相关性区域乘以较大的权重，而低相关性区域标记为较小的权重。权重范围是（0-1）。他是可微的，可以正常进行反向传播。 观察上图，这是一个图像生成标题的任务。上面是soft 下面是hard，我们可以看到，soft attention的权重是每次被放置在整张图像上，注意力关注的部分（越白）的数值越接近1，越黑越接近0 ","date":"2020-12-05","objectID":"/2020/12/attunet/:1:1","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"unet需要attention的原因 在传统的unet中，为了避免在decoder时丢失大量的空间精确细节信息，使用了skip的手法，直接将encoder中提取的map直接concat到decoder相对应的层。但是，提取的low-level feature有很多的冗余信息（刚开始提取的特征不是很好）。 软注意力的使用，可以有效抑制无关区域中的激活，减少冗余的部分的skip。 ","date":"2020-12-05","objectID":"/2020/12/attunet/:1:2","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"Abstract 开题点睛，创新点在于提出了一种注意力门attention gate (AG)模型。用该模型进行训练时，能过抑制模型学习与任务无关的部分，同时加重学习与任务有关的特征。（集中注意力到有用的地方，提取有用的东西，甩掉没用的东西） AG即插即用，可以直接集成到网络模型当中。 ","date":"2020-12-05","objectID":"/2020/12/attunet/:2:0","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"Introduction 在医学分割中，当目标器官形状和大小在不同患者间差异较大时。还是需要多级级联cnn。胆识级联模型中所有模型都会重复的提取相似的低级特征。 如果使用文章所提的attention gate的方法，就可以代替，级联网络的使用。能够注意与学习任务有关的特征。 ","date":"2020-12-05","objectID":"/2020/12/attunet/:3:0","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"Methodogy 文章中用了不少数学公式来讲方法，并且时3D形式的。这里结合2D的代码来看看是怎么实现这个注意力门的。 代码参考：https://github.com/LeeJunHyun/Image_Segmentation def forward(self,x): # encoding path x1 = self.Conv1(x) #1*3*512*512 -\u003econv(3,64)-\u003econv(64,64)-\u003e 1*64*512*512 x2 = self.Maxpool(x1) #1*64*512*512 -\u003e 1*64*256*256 x2 = self.Conv2(x2) #1*64*256*256 -\u003econv(64,128)-\u003econv(128,128)-\u003e 1*128*256*256 x3 = self.Maxpool(x2) #1*128*256*256 -\u003e 1*128*128*128 x3 = self.Conv3(x3) #1*128*128*128 -\u003econv(128,256)-\u003econv(256,256)-\u003e 1*256*128*128 x4 = self.Maxpool(x3)#1*256*128*128 -\u003e 1*256*64*64 x4 = self.Conv4(x4) #1*256*64*64 -\u003econv(256,512)-\u003econv(512,512)-\u003e 1*512*64*64 x5 = self.Maxpool(x4)#1*512*64*64 -\u003e 1*512*32*32 x5 = self.Conv5(x5) #1*512*32*32-\u003econv(512,1024)-\u003econv(1024,1024)-\u003e 1*1024*32*32 # decoding + concat path d5 = self.Up5(x5) #1*1024*32*32 -\u003eUpsample-\u003e 1*1024*64*64 -\u003e conv(1024,512) -\u003e1*512*64*64 x4 = self.Att5(g=d5,x=x4) #2(1*512*64*64) -\u003e 1*1*64*64 -\u003e1*512*64*64 d5 = torch.cat((x4,d5),dim=1) #1*1024*64*64 d5 = self.Up_conv5(d5) #1*1024*64*64 -\u003econv(1024,512)-\u003econv(512,512)-\u003e 1*512*64*64 d4 = self.Up4(d5) x3 = self.Att4(g=d4,x=x3) d4 = torch.cat((x3,d4),dim=1) d4 = self.Up_conv4(d4) d3 = self.Up3(d4) x2 = self.Att3(g=d3,x=x2) d3 = torch.cat((x2,d3),dim=1) d3 = self.Up_conv3(d3) d2 = self.Up2(d3) x1 = self.Att2(g=d2,x=x1) d2 = torch.cat((x1,d2),dim=1) d2 = self.Up_conv2(d2) d1 = self.Conv_1x1(d2) return d1 上面的代码时forward的整体框架，unet的框架就不多做介绍，直接看attention的实现。对于一张输入为1x3x512x512（1是batchsize，3是通道）的2D图,执行到x5的时候（经过五次下采样）已经是最小的feature map了（1x1024x32x32）。对其进行上采样得到d5（1x512x64x64）。 对d5和x4执行Att5(g=d5,x=x4) self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256) class Attention_block(nn.Module): def __init__(self,F_g,F_l,F_int): super(Attention_block,self).__init__() self.W_g = nn.Sequential( nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True), nn.BatchNorm2d(F_int) ) self.W_x = nn.Sequential( nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True), nn.BatchNorm2d(F_int) ) self.psi = nn.Sequential( nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True), nn.BatchNorm2d(1), nn.Sigmoid() ) self.relu = nn.ReLU(inplace=True) def forward(self,g,x): g1 = self.W_g(g) #1x512x64x64-\u003econv(512，256)/B.N.-\u003e1x256x64x64 x1 = self.W_x(x) #1x512x64x64-\u003econv(512，256)/B.N.-\u003e1x256x64x64 psi = self.relu(g1+x1)#1x256x64x64di psi = self.psi(psi)#得到权重矩阵 1x256x64x64 -\u003e 1x1x64x64 -\u003esigmoid 结果到（0，1） return x*psi #与low-level feature相乘，将权重矩阵赋值进去 x4是从上往下下采样得到的图。d5是x4下一层map上采样的图。要对d5和x4执行Att5(g=d5,x=x4)。 图中的xl对应代码中的x4，g代表代码中的d5。（想象一下unet的图，xl就是左边的东西，g是右边的对应大小的东西） g（1x512x64x64）/ x4（1x512x64x64） 注意力模块执行步骤： 对g做1*1卷积得到 1x256x64x64 对xl做1*1卷积得到 1x256x64x64 讲1，2步结果相加（为什么要加起来呢？为了突出特征，如果在两个图中某个点两者都有，加起来，会更为突出） 对第3步结果relu 对第4步结果做conv(256,1)卷积,将256通道降到1通道。得到1x1x64x64的图 对第5步结果进行sigmoid，使得值落在（0，1）区间，值越大，越是重点。（这个得到的就是注意力权重） 这里因为图的大小一样，所以不需要resampler。 最后和xl相乘，把注意力权重赋到low-level feature中。 attention出来的结果在和上采样的结果（x4）进行concat（这里就和unet一样了。区别就是unet是skip的是直接过来的low-level feature，而我这里concat的是low-level feature是先经过注意力机制赋予权重（0-1）的map） 在上图中。在3、6、10和150个epoch时，其中红色突出显示较高的注意力。随着训练的进行，网络学会了专注于期望的区域。 ","date":"2020-12-05","objectID":"/2020/12/attunet/:4:0","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"参考 https://towardsdatascience.com/a-detailed-explanation-of-the-attention-u-net-b371a5590831 ","date":"2020-12-05","objectID":"/2020/12/attunet/:5:0","tags":["语义分割","医学图像","注意力机制","空间注意力"],"title":"【论文笔记】Attention U-Net","uri":"/2020/12/attunet/"},{"categories":["论文笔记"],"content":"Object-Contextual Representations for Semantic Segmentation","date":"2020-11-25","objectID":"/2020/11/ocr/","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"原文：Object-Contextual Representations for Semantic Segmentation 论文链接: https://arxiv.org/abs/1909.11065 pytorch official code: https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR 笔记时间：2020.11.25 ","date":"2020-11-25","objectID":"/2020/11/ocr/:0:0","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"写在前面 文章发表在了ECCV2020 微软和中科大提出的HRNet可以保持高分辨率的特性可以用于对位置信息敏感的任务中。 在语义分割的任务中进一步的提出了一个聚合上下文信息的模块。也就是今天说的这个OCR。 看到目前的分割文章都有一种A+B的感觉，A是一个backbone（resnet,inception）等等。B是一个语义聚合的模块（包括pspnet中的PPM，deeplab中的ASPP）等等 ","date":"2020-11-25","objectID":"/2020/11/ocr/:1:0","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"Abstract 文章的总体思路是：像素的标签是像素所在的对象的标签，并且通过用相应的对象区域表示来表征每个像素来加强像素表示。 主要分为三个步骤： First, we learn object regions under the supervision of the ground-truth segmentation.（这里的object regions可以大概理解成我先搞一个粗略的分割） Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations. 具体看这些步骤好像很乱，事实上大概目标就是计算像素和对象之间的关系，最后来加强每个像素的表示。 ","date":"2020-11-25","objectID":"/2020/11/ocr/:2:0","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"Introduction+Related work 对于上下文信息的利用最早就有aspp和ppm这种具有固定区域结构的空间表示。但是本文提出的OCR对比固定空间表示的上下文聚合更具灵活性。可以看到下图： 对于aspp来讲，每个像素去提取上下文信息都是固定空间位置的。对于OCR来讲，他提取的上下文信息是提取目标像素所在对象的上下文信息。 论文整体的思路有点有粗到细的感觉。先根据粗的分割结果表示出对象。再通过对象和像素之间的关系，加强表示像素的信息。但是这个粗分割的结果只用于生成上下文信息。 ","date":"2020-11-25","objectID":"/2020/11/ocr/:3:0","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"Approach 文章这个部分写了很多的公式。直接看公式可能有点云里雾里，直接看代码可能更清楚些。 看的是上面这个文件的代码 https://github.com/HRNet/HRNet-Semantic-Segmentation/blob/HRNet-OCR/lib/models/seg_hrnet_ocr.py 633行之前都是进行HRNet，从633行开始才是OCR的部分 # ocr #这句的作用是进行粗分割，形成对应论文中的软对象区域，也就是粉色框里的东西 out_aux = self.aux_head(feats) # compute contrast feature #这里feats就是图中的蓝色框里的东西，对应论文中的pixel reprations feats = self.conv3x3_ocr(feats) #这里形成的是object region representation，就是紫色框里的东西 context = self.ocr_gather_head(feats, out_aux) #计算pixel和object region的关系。最后形成的是augmented #representation也就是黄色框里的深蓝色块（这步里面进去看有很多的步骤） feats = self.ocr_distri_head(feats, context) #最后分类到对应的channel数 out = self.cls_head(feats) 在看代码的过程中没有看到最后上采样4倍的地方，找了好久，最后发现好像写在了criterion.py 交叉熵损失那里，不知道为啥要这么做。 对比实验验证对象区域监督对性能有不小的影响 ","date":"2020-11-25","objectID":"/2020/11/ocr/:4:0","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"Experiments 训练:用于监督目标区域的损失设置损失权重为0.4。进行了数据增强。 作者不仅在性能上进行了对比，在内存使用，参数量等也进行了对比。 ","date":"2020-11-25","objectID":"/2020/11/ocr/:5:0","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"Conclusions 文章提出了一种面向语义分割的对象上下文表示方法。主要的想法就是认为像素的标签是像素所在的对象的标签，并且通过用相应的对象区域表示来表征每个像素来加强像素表示。 ","date":"2020-11-25","objectID":"/2020/11/ocr/:6:0","tags":["语义分割","注意力机制"],"title":"【论文笔记】OCR","uri":"/2020/11/ocr/"},{"categories":["论文笔记"],"content":"Deep high-resolution representation learning for visual recognition ","date":"2020-11-19","objectID":"/2020/11/hrnet/","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"原文：Deep high-resolution representation learning for visual recognition 论文链接: https://arxiv.org/abs/1908.07919v2 pytorch official code: https://github.com/HRNet 笔记时间：2020.11.22 文章最早发表在了CVRP2019，后面被顶刊TPAMI录用。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:0:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"写在前面 之所以要看这篇文章，是先看了OCR，看代码的过程中碰到backbone是HRNet，HRNet搭配OCR达到了很好的结果。 对目前所看到的语义分割相关的文章中可以发现，对于一个语义分割任务，首先会通过一个backbone获得一个分辨率较小的图（很多论文都会提到output stride，即输入图像尺寸经过一个网络后的尺寸的大小的比例），再对这个分辨率较小的图进行一些利用上下文语义信息的处理。 backbone的任务不仅仅适用于语义分割，最早适用于分类网路的，同时在各种计算机视觉的任务中都是基本操作。由此，诞生了一些重要的网络，例如残差的resnet，轻量级的googlenet，vgg等等，同时也包括这篇HRNet。对于上下文语义处理的步骤，最早也是出现了包括deeplab和pspnet两个经典的方法，后面也有利用注意力机制的no-local和ccnet等等。包括和HRNet搭配使用的这个OCR。 HRNet这是一篇SOTA的文章。对于视觉识别任务，包括姿态估计，语义分割等。一般的方法都是使用卷积神经网络进行不断地降采样，包括resnet和vggnet等，然后再恢复高分辨率。而HRnet的特点在于把串行的结构做成并行的，把降低分辨率的操作改成保持分辨率的操作。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:1:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"Abstract 两个关键特点： 1.高分辨率和低分辨率并行连接，同步推进。 2.高低分辨率图之间不断地交换信息 高分辨率图的存在使得空间上更加精准，低分辨率图的存在使得语义上更充分。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:2:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"Introduction 对于一般的分类网络来讲，通过卷积逐渐缩小图像的空间尺寸，进一步用于分类。 对于位置敏感的计算机视觉任务是需要高分辨率表示的。hrnet在整个过程中保持高分辨率的表示。 网络由四个阶段组成。第n个阶段包含对应于n个分辨率的n个流。通过反复的交换平行流中的信息来进行重复进行多分辨率的融合。 其他的高低分辨率融合都是通过融合low_level的高分率和低分辨率上采用获得的high_level高分辨率。而hrnet是在低分辨率的帮助下，多次融合高分辨率。 HRNetV1：只输出从高分辨率卷积流计算的高分辨率表示。 HRNetV2：结合了所有从高到底分辨率的并行流的表示。 HRNetV2p：从HRNetV2的高分辨率输出构建出multi-level representation。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:3:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"Related work 学习低分辨表示：以FCN为代表，移除分类网络的全连接层。得到的低分辨率表示来获得粗略估计图，通过结合low_level的中分辨率层来达到相对精细的分割。之后的改进包括deeplab和pspnet。 恢复高分辨率表示：通过上采用过程来恢复高分辨率表示，segnet，unet，encoder-decoder，不对称上采样等等。 保持高分辨率表示 多尺度融合 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:4:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"Model 先通过2个3*3卷积降到1/4的resolution。 由几部分组成： parallel multi-resolution convolutions repeated multi-resolution fusions representation head ","date":"2020-11-19","objectID":"/2020/11/hrnet/:5:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"1.parallel multi-resolution convolutions 用一个并行卷积流的方法，从第一阶段开始，逐步逐个添加高分辨到低分辨率的流。后一个阶段的并行流的分辨率由前一个阶段的分辨率和更低分辨率组成。 看论文这段话说的感觉复杂，其实看图可能更好理解一点。说白了就是有很多个阶段，越往后面，不同分辨的数量越多。在第一阶段就只有原尺寸的图，第二阶段就有两个不同分辨率图的并行继续，以此类推。 上图中N32表示的就是第三阶段的第二个流的表示。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:5:1","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"2.repeated multi-resolution fusions 重复融合多分辨率的模块，跨分辨率交换信息。 这是一个融合三分辨率的例子。可以看出三个输出中的每一个输出都是与三个输入相关的，即$R^o_r= f_{1r}(R^i_1)+f_{2r}(R^i_2)+f_{3r}(R^i_3)$ 同时也会得到一个额外的输出，$R^o_4= f_{14}(R^i_1) + f_{24}(R^i_2) + f_{34}(R^i_3) $ 这些个f就是一系列操作，也就是图中所示的卷积上采样等操作。对高分辨率到低分辨率，低分辨率到高分辨率，同分辨率到同分辨率，操作均不同，具体可见上图。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:5:2","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"3.representation head 有三种不同的输出表示： 对于最后的结果的四分辨率流，根据如何去利用这个流分成了三种不同的方式。 (a):只输出高分辨率 （人体姿态估计） (b):拼接四个流的输出 （语义分割） (c):在b的基础上形成特征金字塔表示（对象检测） ","date":"2020-11-19","objectID":"/2020/11/hrnet/:5:3","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"4.组装起来 再次回过头以完整和组装的视角来看这张图的时候，会更清晰一些。网络的机构体现了最初摘要中所说的并行的意思。有并行卷积流同步的向前推进。上图结构分为4个stage，每个stage的每个分辨率都要先经过四次残差卷积。一个stage中，通过3*3的卷积操作使得从高分辨率到低分辨率。分辨率越小越宽（channel数越多）。呈现2的指倍数增长，最小的分辨率的宽度是最大的八倍。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:5:4","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"语义分割的应用 这个方法可以用在很多计算机视觉领域，我只看了语义分割的部分。 ​ 就像上图所示，对四个分辨率的输出进行拼接。这就是就一个维度为15C(C是最大的那个分辨率的channel数，1+2+4+8=15) 对其进行softmax再上采样四倍得到与原图一样大小的分割图。 可见，在各大主流的数据集上都体现了HRNet+ORC的强势 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:6:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"Conclusions 作者总结了HRNet和其他的不同。高低分辨率是并联而不是串联，高分辨率是remain的而不是recover的，具有 strong position sensitivity（对位置敏感的任务好）。 将来的主要工作是希望将HRNet运用到各个计算机视觉的任务中。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:7:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"关于代码 github中给出了HRNet+OCR相应的代码。我是直接看的HRNet-OCR分支下的这个文件。 对于HRNet而言，代码中有四个比较重要的类，BasicBlock、Bottleneck、HighResolutionModule、HighResolutionNet四个类。BasicBlock和Bottleneck是残差块，在resnet中也是能看到的。HighResolutionModule是进行多分辨率融合的模块，HighResolutionNet是HRNetv2。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:8:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"1.残差块BasicBlock和Bottleneck 左边是BasicBlock，右边是Bottleneck。在resnet中，左图是resnet-18/34使用的，右图是resnet50/101/152使用的。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:8:1","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"2.BasicBlock(左图) class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.relu = nn.ReLU(inplace=relu_inplace) self.conv2 = conv3x3(planes, planes) self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out = out + residual out = self.relu(out) return out 基本结构就是对应着左图来看 对于输入的参数，inplanes是输入维度, planes是第一个卷积的输出维度, stride和downsample来看resolution要不要下降。 跳层连接：当模块输入的分辨率与经过卷积处理的分辨率一致时，直接相加；当不一致时（stride！=1）需要使用downsample降低输入的分辨率再相加。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:8:2","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"3.Bottleneck(右图) class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False) self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False) self.bn3 = BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM) self.relu = nn.ReLU(inplace=relu_inplace) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out = out + residual out = self.relu(out) return out 与BasicBlock基本相似，深度更深一些。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:8:3","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"4.HighResolutionModule 这个类的功能是对每一个分辨率表示的分支进行特征提取。当只有一个分辨率分支时，就没有融合模块，直接返回结果。当有多个分支流的时候就需要先对各个分支进行计算，最后执行融合过程。 代码有点长，分解各个def来分析: 4.1 _check_branches 用来检查 4.2 _make_one_branch 对一个分支进行特征提取（对应下图中一个红框的部分）。在单个分支中，特征提取使用到数目为num_blocks的basicblock或者bottleblock（实际在开源代码中stage1是bottleblock，satge2-stage是basicblock） 先判断是否会downsample，写downsample模块（用在basicblock中） 搭建4个block，第一个block有可能会降维，后面3个block完全一致。 4.3 _make_branches 循环调用上面说的_make_one_branch函数，比如并行三列的话，就要调用三次。 4.4 _make_fuse_layers 进行低分辨率和高分辨率的融合。 如果只有一行，那就不用融合。 如果有并行结构，就要进行特征融合，以论文中给出的结构为例，这是要一个三分辨率融合至三分辨率的过程。 函数中嵌入了一个双层循环：一个变量i，一个变量j 如果i\u003cj：那么，所有j分支都要上采样到和i分支一样分辨率。上采样的倍数即为：2^(j-i)倍 如果i=j：就是他本身 如果i\u003ej：那么高分辨率的分支要到卷积下采样和i一样分辨率大小。这里又嵌套了一个循环k，是因为跨层下采样经过的卷积次数不一样，最后一次卷积不能加rule。 class HighResolutionModule(nn.Module): def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels, fuse_method, multi_scale_output=True): super(HighResolutionModule, self).__init__() self._check_branches( num_branches, blocks, num_blocks, num_inchannels, num_channels) self.num_inchannels = num_inchannels self.fuse_method = fuse_method self.num_branches = num_branches self.multi_scale_output = multi_scale_output self.branches = self._make_branches( num_branches, blocks, num_blocks, num_channels) self.fuse_layers = self._make_fuse_layers() self.relu = nn.ReLU(inplace=relu_inplace) def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels): if num_branches != len(num_blocks): error_msg = 'NUM_BRANCHES({}) \u003c\u003e NUM_BLOCKS({})'.format( num_branches, len(num_blocks)) logger.error(error_msg) raise ValueError(error_msg) if num_branches != len(num_channels): error_msg = 'NUM_BRANCHES({}) \u003c\u003e NUM_CHANNELS({})'.format( num_branches, len(num_channels)) logger.error(error_msg) raise ValueError(error_msg) if num_branches != len(num_inchannels): error_msg = 'NUM_BRANCHES({}) \u003c\u003e NUM_INCHANNELS({})'.format( num_branches, len(num_inchannels)) logger.error(error_msg) raise ValueError(error_msg) def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1): downsample = None if stride != 1 or \\ self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.num_inchannels[branch_index], num_channels[branch_index] * block.expansion, kernel_size=1, stride=stride, bias=False), BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM), ) layers = [] layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample)) self.num_inchannels[branch_index] = \\ num_channels[branch_index] * block.expansion for i in range(1, num_blocks[branch_index]): layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index])) return nn.Sequential(*layers) def _make_branches(self, num_branches, block, num_blocks, num_channels): branches = [] for i in range(num_branches): branches.append( self._make_one_branch(i, block, num_blocks, num_channels)) return nn.ModuleList(branches) def _make_fuse_layers(self): if self.num_branches == 1: return None num_branches = self.num_branches num_inchannels = self.num_inchannels fuse_layers = [] for i in range(num_branches if self.multi_scale_output else 1): fuse_layer = [] for j in range(num_branches): if j \u003e i: fuse_layer.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False), BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM))) elif j == i: fuse_layer.append(None) else: conv3x3s = [] for k in range(i-j): if k == i - j - 1: num_outchannels_conv3x3 = num_inchannels[i] conv3x3s.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), BatchNorm2d(num_outchannels_conv3x3, momentum=BN_MOMENTUM))) else: num_outchannels_conv3x3 = num_inchannels[j] conv3x3s.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), BatchNorm2d(num_outchannels_conv3x3, momentum=BN_MOMENTUM), nn.ReLU(inplace=relu_inplace))) fuse_layer.","date":"2020-11-19","objectID":"/2020/11/hrnet/:8:4","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"5.HighResolutionNet 这就是最后执行网络的地方。 这里同时也包括了OCR的内容。 就HRNet而言，具体过程可如下： 原图先降成1/4大小 执行1个stage1（4个block） 通过卷积生成1/2分辨率的流（现在有两条流） 执行1个stage2（两个流的4个block以及两个流之间交融） 通过卷积生成1/4分辨率的流（现在有三条流） 执行4个stage3（三个流的4个block以及三个流之间交融） 通过卷积生成1/8分辨率的流（现在有四条流） 执行3个stage4（四个流的4个block以及四个流之间交融） 上采样下面三条流，使之大小变回原大小，在concat拼接channel用于后续分割任务 class HighResolutionNet(nn.Module): def __init__(self, config, **kwargs): global ALIGN_CORNERS extra = config.MODEL.EXTRA super(HighResolutionNet, self).__init__() ALIGN_CORNERS = config.MODEL.ALIGN_CORNERS # stem net self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False) self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM) self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False) self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM) self.relu = nn.ReLU(inplace=relu_inplace) self.stage1_cfg = extra['STAGE1'] num_channels = self.stage1_cfg['NUM_CHANNELS'][0] block = blocks_dict[self.stage1_cfg['BLOCK']] num_blocks = self.stage1_cfg['NUM_BLOCKS'][0] self.layer1 = self._make_layer(block, 64, num_channels, num_blocks) stage1_out_channel = block.expansion*num_channels self.stage2_cfg = extra['STAGE2'] num_channels = self.stage2_cfg['NUM_CHANNELS'] block = blocks_dict[self.stage2_cfg['BLOCK']] num_channels = [ num_channels[i] * block.expansion for i in range(len(num_channels))] self.transition1 = self._make_transition_layer( [stage1_out_channel], num_channels) self.stage2, pre_stage_channels = self._make_stage( self.stage2_cfg, num_channels) self.stage3_cfg = extra['STAGE3'] num_channels = self.stage3_cfg['NUM_CHANNELS'] block = blocks_dict[self.stage3_cfg['BLOCK']] num_channels = [ num_channels[i] * block.expansion for i in range(len(num_channels))] self.transition2 = self._make_transition_layer( pre_stage_channels, num_channels) self.stage3, pre_stage_channels = self._make_stage( self.stage3_cfg, num_channels) self.stage4_cfg = extra['STAGE4'] num_channels = self.stage4_cfg['NUM_CHANNELS'] block = blocks_dict[self.stage4_cfg['BLOCK']] num_channels = [ num_channels[i] * block.expansion for i in range(len(num_channels))] self.transition3 = self._make_transition_layer( pre_stage_channels, num_channels) self.stage4, pre_stage_channels = self._make_stage( self.stage4_cfg, num_channels, multi_scale_output=True) last_inp_channels = np.int(np.sum(pre_stage_channels)) ocr_mid_channels = config.MODEL.OCR.MID_CHANNELS ocr_key_channels = config.MODEL.OCR.KEY_CHANNELS self.conv3x3_ocr = nn.Sequential( nn.Conv2d(last_inp_channels, ocr_mid_channels, kernel_size=3, stride=1, padding=1), BatchNorm2d(ocr_mid_channels), nn.ReLU(inplace=relu_inplace), ) self.ocr_gather_head = SpatialGather_Module(config.DATASET.NUM_CLASSES) self.ocr_distri_head = SpatialOCR_Module(in_channels=ocr_mid_channels, key_channels=ocr_key_channels, out_channels=ocr_mid_channels, scale=1, dropout=0.05, ) self.cls_head = nn.Conv2d( ocr_mid_channels, config.DATASET.NUM_CLASSES, kernel_size=1, stride=1, padding=0, bias=True) self.aux_head = nn.Sequential( nn.Conv2d(last_inp_channels, last_inp_channels, kernel_size=1, stride=1, padding=0), BatchNorm2d(last_inp_channels), nn.ReLU(inplace=relu_inplace), nn.Conv2d(last_inp_channels, config.DATASET.NUM_CLASSES, kernel_size=1, stride=1, padding=0, bias=True) ) def _make_transition_layer( self, num_channels_pre_layer, num_channels_cur_layer): num_branches_cur = len(num_channels_cur_layer) num_branches_pre = len(num_channels_pre_layer) transition_layers = [] for i in range(num_branches_cur): if i \u003c num_branches_pre: if num_channels_cur_layer[i] != num_channels_pre_layer[i]: transition_layers.append(nn.Sequential( nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False), BatchNorm2d( num_channels_cur_layer[i], momentum=BN_MOMENTUM), nn.ReLU(inplace=relu_inplace))) else: transition_layers.append(None) else: conv3x3s = [] for j in range(i+1-num_branches_pre): inchannels = num_channels_pre_layer[-1] outchannels = num_channels_cur_layer[i] \\ if j == i-num_branches_pre else i","date":"2020-11-19","objectID":"/2020/11/hrnet/:8:5","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"手绘流程图： 1.原图先进去，先降成1/4大小。 2.执行1个stage1（4个block） 3.分支到两个流 4.执行1个stage2（两个流的4个block以及两个流之间交融） 后面其实都很类似，就不放上来了。 ","date":"2020-11-19","objectID":"/2020/11/hrnet/:9:0","tags":["语义分割","人体姿态估计","高分辨率"],"title":"【论文笔记】HRNet","uri":"/2020/11/hrnet/"},{"categories":["论文笔记"],"content":"Fully Convolutional Networks for Semantic Segmentation ","date":"2020-09-27","objectID":"/2020/09/fcn/","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"原文：Fully Convolutional Networks for Semantic Segmentation 论文链接: https://arxiv.org/abs/1411.4038 https://arxiv.org/abs/1411.4038v2 Caffe 官方实现:https://github.com/shelhamer/fcn.berkeleyvision.org **笔记时间：**2020.9.27 可能存在的改进点（就当时而言）： ①拼接的方法不一定要点位相加，可以尝试像u-net一样的channel拼接。 ②对类别之间关系进行联系，条件随机场。 ","date":"2020-09-27","objectID":"/2020/09/fcn/:0:0","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"主要思路和特定 1.使用全卷积（没有全连接层）的网络：改造AlexNet，the VGG ，GoogLeNet。文章和代码主要是对VGG进行改进。 2.输入的图像可以是任意尺寸的。 3.上采样使用了反卷积的技术 4.有skip结构，对信息进行了融合（融合的方式有直接逐点相加，通道中拼接。这里是用的直接逐点相加） ","date":"2020-09-27","objectID":"/2020/09/fcn/:1:0","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"对传统cnn网络的改变 传统的cnn模型，卷积之后会展开成向量进行全连接网络，最后都是会输出一个向量，用来判别该图像属于某一类（是对图像层面上的一个分类）。而FCN全程都是卷积，最后通过上采样会得到一个与原图一样大的图，实现对每一个pixel的分类。 从上图就可以看出CNN最后是要判断这个图是什么类的。而FCN想要给出具体像素的类别所属。 ","date":"2020-09-27","objectID":"/2020/09/fcn/:2:0","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"网络结构 网络结构主要还是对AlexNet进行了一个改造，把最后两个全连接层改成了卷积层。 其实在论文中，达到最好效果的网络是VGG16，此处可能使用AlexNet便于绘图。 蓝色块代表conv，绿色代表max pooling，灰色是裁剪，橙色是deconv，黄色是融合。 论文中也有提到，如果直接使用高维特征图进行上采样，得到的效果并不会很好。所以上采样的过程需要慢慢进行并融合低维度的特征图。例如上图所示，如果直接将16x16的最高维的图上采样变成500x500不会有好的结果。要先对16x16的特征图做一次\"小型的\"上次采样，得到34x34的图，再与前面的低维特征图进行一定的融合，再进行后面的操作。 模型最后的输出是21张变为原图大小的图片。为了对每个像素进行分类（看看这个像素属于21类中的哪一类），对逐个像素进行在不同分类上的概率值（就是该像素对应在21张图上的值）进行求解，最大的值便是该像素属于的分类。 ","date":"2020-09-27","objectID":"/2020/09/fcn/:3:0","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"上采样-反卷积 上采样的方法一般有插值法、反卷积、反池化。本文使用的是反卷积。 反卷积（deconvolution） 对于上图来说，反卷积的输入是下面的部分，反卷积的输出是上面的部分。阴影部分是卷积核。 操作的基本流程就是先对输入先进行padding，在进行常规的卷积操作即可。关于反卷积的细节可以参考： 《Up-sampling with Transposed Convolution》 关于转置卷积矩阵的参数随着训练过程不断被优化，但是它是在随机初始化的基础上进行优化，还是在原始卷积矩阵的基础上进行优化？ 答：根据pytorch的源码，是随机生成的符合尺寸的就行。 ","date":"2020-09-27","objectID":"/2020/09/fcn/:4:0","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"skip结构 对原图进行卷积conv1、pool1后图像缩小为1/2；对图像进行第二次卷积conv2、pool2后图像缩小为1/4；对图像进行第三次卷积conv3、pool3后图像缩小为1/8，此时保留pool3的feature map3；对图像进行第四次卷积conv4、pool4后图像缩小为1/16，此时保留pool4的feature map4；对图像进行第五次卷积conv5、pool5后图像缩小为1/32，然后把原来CNN操作过程中的全连接改成卷积操作的conv6、conv7，图像的feature map5的大小依然为原图的1/32。将feature map5的上采样结果和feature map4进行融合，得到feature map4.5。再将feature map4.5的上采样结果feature map3进行融合得到1/8原始图像大小的图，将这个图进行8倍的上采样得到与原图一样像素大小的图。实现像素级分类的结果。 ","date":"2020-09-27","objectID":"/2020/09/fcn/:5:0","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"Conclusion Fully convolutional networks are a rich class of models, of which modern classification convnets are a special case. Recognizing this, extending these classification nets to segmentation, and improving the architecture with multi-resolution layer combinations dramatically improves the state-of-the-art, while simultaneously simplifying and speeding up learning and inference. ","date":"2020-09-27","objectID":"/2020/09/fcn/:6:0","tags":["语义分割"],"title":"【论文笔记】FCN","uri":"/2020/09/fcn/"},{"categories":["论文笔记"],"content":"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"**原文：**Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 论文链接: https://arxiv.org/abs/1802.02611 tensorflow 官方实现: https://github.com/tensorflow/models/tree/master/research/deeplab pytorch github高星: https://github.com/jfzhang95/pytorch-deeplab-xception **笔记时间：**2020.9.23 这是v3+，前面应该是有v1-v3的。 （v1：修改了VGG16引入空洞卷积，v2：设计ASPP模块，v3：串联ASPP与并联ASPP，讨论了3x3卷积可能会丢失部分信息，提出了1x1卷积的必要性。v3+：使用小幅度修改的xception主网络，结合deeplab v3+作为encoder，自己设计decoder） 可能可以改进点（对于当时）： ①encoder拼接处本文是直接拼接的，可能可以采用其他的拼接手法（例如提取最重要的几个拼一下） ②上采样使用反卷积不知道效果会不会有提升 ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:0:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Abstract **金字塔模型（Atrous Spatial Pyramid Pooling，ASPP）：**可以提取多尺度特征信息（通过对输入进行不同rates不同观察域的提取，即atrous convolution ）。 **encode-decoder：**在获取边界信息上更加有效。 本文结合两者的优点，提出了一种encoder-decoder，其中把DeepLabv3作为encoder，另外提出一个简单的结构作为decoder（为更好的提取边界信息）。 本文在建立encoder和decoder的过程中利用了对Xception和depthwise separable convolution ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:1:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Introduction 说了一下大背景，即用神经网络和卷积神经网络的方法解决图像问题。 主要思想：In this work, we consider two types of neural networks that use spatial pyramid pooling module [18,19,20] or encoder-decoder structure [21,22] for semantic segmentation, where the former one captures rich contextual information by pooling features at different resolution while the latter one is able to obtain sharp object boundaries. PSPNet：虽然能提供丰富的语意信息，边界的细节信息却会丢失。 ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:2:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Related Work (FCNs)–(PSP/CRF)–(本文) ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:3:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Methods 介绍了Atrous convolution、Depthwise separable convolution、 **encoder:**先是通过xception使得output strid=16，再进行ASPP。 ​ **ASPP：**一个1x1卷积 + 三个3x3卷积（rate={6，12，18}）+全局的平均池化 **decoder：**先使encoder结果（bilinear）上采样4倍，再与encoder中出来的low_level_feature进行cat。在3x3卷积，再上采用4倍。 ps：期间对 low_level_feature有个1x1降低通道的操作。 **depthwise separable convolution：**减小卷积的参数数量和计算量 Xception的改变： 更深的Xception结构， middle flow从迭代8次到迭代16次 所有max pooling结构被stride=2的深度可分离卷积替代 在3x3卷积后面额外的加上batch normalization标准化和ReLU激活 ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:4:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Experimental Evaluation 用改进的Xception和ResNet-101预训练，讲了一下怎么训练。在各种测试集上的效果。 decoder计： 使用1x1卷积减少通道数目 使用3x3卷积来获取分割结果 使用了encode中半路提取的low-level features（128x128x128）用来进行cat ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:5:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Conclusion “Our proposed model \\DeepLabv3+ employs the encoder-decoder structure where DeepLabv3 is used to encode the rich contextual information and a simple yet effective decoder module is adopted to recover the object boundaries. One could also apply the atrous convolution to extract the encoder features at an arbitrary resolution, depending on the available computation resources. We also explore the Xception model and atrous separable convolution to make the proposed model faster and stronger. Finally, our experimental results show that the proposed model sets a new state-of-the-art performance on PASCAL VOC 2012 and Cityscapes datasets.\" ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:6:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Related Concepts ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:7:0","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"1.depthwise separable convolution 例：同样要实现卷积，普通的卷积和深度可分离卷积的区别。深度可分离卷积拥有更小的参数量和计算量。 这里，普通卷积的参数量128x3x3x3=3456，而深度可分离卷积为3x3x3+1x1x3x128=411 « 3456 ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:7:1","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"2.atrous convolution 扩张率(dilation rate) 在pytorch中的cov2d函数中可以设置dilation参数来进行。该参数定义了卷积核处理数据时各值的间距。 上图所示的dilation=2 ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:7:2","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"3.改进版本的Xception 更深的Xception结构， middle flow从迭代8次到迭代16次 所有max pooling结构被stride=2的深度可分离卷积替代 在3x3卷积后面额外的加上Batch Normalization标准化和ReLU激活 ","date":"2020-09-23","objectID":"/2020/09/deeplabv3p/:7:3","tags":["语义分割","Deeplab"],"title":"【论文笔记】Deeplab v3+","uri":"/2020/09/deeplabv3p/"},{"categories":["论文笔记"],"content":"Densely Connected Convolutional Networks","date":"2020-09-09","objectID":"/2020/09/densenet/","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"原文：Densely Connected Convolutional Networks 论文链接: https://arxiv.org/abs/1608.06993 pytorh high-star 复现：https://github.com/gpleiss/efficient_densenet_pytorch 笔记时间：2020.09.09 文章收录于在CVPR2017(Best Paper Award) ","date":"2020-09-09","objectID":"/2020/09/densenet/:0:0","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"写在前面 经典文章densenet，cvpr2017的best paper，16年是resnet。 densenet主要还是去对比resnet和inception。resnet是做深，inception是做宽。densenet是利用feature去做文章。 ","date":"2020-09-09","objectID":"/2020/09/densenet/:1:0","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"Abstract 网络结构：靠前的层的输出作为所有靠后层的输入 网络优点： 减轻梯度消失 增强特征传播 加强特征复用 减少权重参数 ","date":"2020-09-09","objectID":"/2020/09/densenet/:2:0","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"Introduction 硬件和网络结构发展促使cnn可以变深。cnn的加深会出现问题，当输入信息经过诸多层到达终点时，可能会消失。大部分的解决方法都是类似resnet那样设置短路径解决的。 文章提出的densenet，每个layer之间都有连接。靠前的层的输出作为所有靠后层的输入。 与resnet不同：renset的拼接采用的是summation的做法，而densenet对特征图聚合是进行concat。 ","date":"2020-09-09","objectID":"/2020/09/densenet/:3:0","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"可以减少权重参数 从直观看，这种稠密的连接，好像会带来相当大的参数量。事实上与传统的卷积网络相比他的参数量更少，文章说是因为它不需要学习冗余的特征图。 对于传统的cnn网络，通过不断地卷积来获取high-level features 他在l层的输出是前一层的输出的卷积结果： 即：对于resnet，利用了恒等映射。可以看作是一个状态经过resnet时，可以是保留的（恒等映射了），也可以是载入网络了并到达下一个状态。 他在l层的输出来自前一层输出的卷积结果和前一层输出本身 即：对于densenet，每一层的输入都整合了前面所有的层的输入，并将自己的输出特征图传递给后续所有层。 他在l层的输出来自前面所有输入的结果， 即：这些公式里的Hl就是代表一系列操作（卷积池化bn激活之类） 由于每一层都接受前面的feature map，网络会更加紧凑，通道的数量可以更少。所以参数量会少，下面的动图可以展示的很好。 每一次卷积的结果都是固定的channel=k，然后concat上前面的feature map。于是每一次concat结果的channel数量都比前面多k。 ","date":"2020-09-09","objectID":"/2020/09/densenet/:3:1","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"提升了信息和梯度的传播 ","date":"2020-09-09","objectID":"/2020/09/densenet/:3:2","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"DenseNets DenseNet的网络结构主要由DenseBlock和Transition组成。 由于需要对不同的层的feature map进行concat操作，所以feature map的size需要一样。于是就需要降采样的操作，于是就需要Transition。 在同一个Denseblock中，feature size保持相同大小，在不同Denseblock之间设置transition layers实现Down sampling ","date":"2020-09-09","objectID":"/2020/09/densenet/:4:0","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"DenseBlock 在denseblock当中，执行的操作是先做B.N，再做ReLu，再做一个3x3的卷积。（BN+ReLU+3x3 Conv） 在每一个denseblock中，他的输出结果都是k通道的，k在DenseNet中称为growth rate，是一个超参数。如果输入的特征图的通道数是k0。那么，它的输出通道数就是k0+k。但是事实上，只有k个是属于这个次卷积操作的输出结果。k0是前面叠加过来的。 随着层数的叠加，通道的数量会越来越大，于是就考虑改进DenseBlock。通过引入bottleneck层来减少计算。 原结构：BN+ReLU+3x3 Conv 改进结构：BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv 主要就是引入了1*1的卷积操作来降低通道，其中1x1 Conv得到的是 4k channel。 ","date":"2020-09-09","objectID":"/2020/09/densenet/:4:1","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"Transition Transition由一个1x1卷积操作和一个pooling操作实现了尺寸的缩小。它连接两个相邻的denseblock，并降低特征图大小 结构为BN+ReLU+1x1 Conv+2x2 AvgPooling 这个1x1的卷积在这里起到什么作用呢？ 压缩模型，假设denseblock的输出是一个n维channel的特征图，通过这个1x1卷积可以让他变成kn维channel，其中k在0到1之间，文中使用了0.5。也就是说，在Transition的过程中不仅特征图大小会变小，维度也会减半。 ","date":"2020-09-09","objectID":"/2020/09/densenet/:4:2","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"代码 由于densenet对内存的要求高，不少复现都采取了一些策略来降低内存使用，当时同时也降低了训练速度。这里找了一个官方github推荐的pytorch实现：https://github.com/gpleiss/efficient_densenet_pytorch 先看看denselayer部分 class _DenseLayer(nn.Module): def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, efficient=False): super(_DenseLayer, self).__init__() self.add_module('norm1', nn.BatchNorm2d(num_input_features)), self.add_module('relu1', nn.ReLU(inplace=True)), self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)), self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)), self.add_module('relu2', nn.ReLU(inplace=True)), self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)), self.drop_rate = drop_rate self.efficient = efficient def forward(self, *prev_features): bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1) if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features): bottleneck_output = cp.checkpoint(bn_function, *prev_features) else: bottleneck_output = bn_function(*prev_features) new_features = self.conv2(self.relu2(self.norm2(bottleneck_output))) if self.drop_rate \u003e 0: new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) return new_features class _DenseBlock(nn.Module): def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, efficient=False): super(_DenseBlock, self).__init__() for i in range(num_layers): layer = _DenseLayer( num_input_features + i * growth_rate, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate, efficient=efficient, ) self.add_module('denselayer%d' % (i + 1), layer) def forward(self, init_features): features = [init_features] for name, layer in self.named_children(): new_features = layer(*features) features.append(new_features) return torch.cat(features, 1) ","date":"2020-09-09","objectID":"/2020/09/densenet/:5:0","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":["论文笔记"],"content":"参考 Review: DenseNet — Dense Convolutional Network (Image Classification) | by Sik-Ho Tsang | Towards Data Science ","date":"2020-09-09","objectID":"/2020/09/densenet/:6:0","tags":["骨干网络"],"title":"【论文笔记】DenseNet","uri":"/2020/09/densenet/"},{"categories":null,"content":"Web Name: Jinhaha’blog Birthday: 2020/09/02 Front-end: Github Page Back-end: Hugo Realm name: Temporary domain name: jqc8438.top Me Name: Jinhaha University: ECNU Doing: Image Processing ; Deep Learning ; 一个从机械转专业到电气，研究生读计算机的cv炼丹师~~~ TODO ","date":"2020-08-02","objectID":"/about/:0:0","tags":null,"title":"关于博客和我","uri":"/about/"},{"categories":["浙江高考志愿辅导"],"content":"211、985院校异地校区（分校）解析","date":"2020-06-11","objectID":"/2020/06/yidi985211/","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"211、985院校大都是传统国家重点高校。教育的发展同经济、社会等其他发展一样，有其自身的规律，传统高校大都建在城市中心，进一步在市中心扩建受到场地限制。随着前些年高校的合并、扩建浪潮。不少985、211高校在同一所城市设有不同的校区，甚至在异地建设了校区或分校。 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:0:0","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"1、与本部的差距 不同的高校有各自不同的特色重点学科。一所大学在同城市的不同校区，因发展历史、师资力量、硬件设施、共享资源等不同，也会有差别。异地建设的校区，大都建设历史较短，与本部比差距会更大点，录取分数也相对低些。 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:0:1","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"2、学籍与两证一书 （证书一般不会出现xx校区字样） 两证（毕业证、学位证）一书（录取通知书）同等规格，学籍管理按教育部规定属地管理，校区的校园就在属地，想藏也藏不住；校区属地的城市名字也记录在档案里，想抹也抹不去。一校多区，谁也否定不了，同一个学校，不同校区，发同样的毕业证，教育部同意、学校愿意、学子乐意。其他旁观者，用不着担心失意。 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:0:2","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"3、与本部差距的原因 一是地域因素，异地建校的选址，北京、深圳、青岛、苏州等大高上知名城市，容易吸引人才，老师也乐意去，考生也愿意去，与本部差距就小点。 二是办学定位，校区定位准确、鲜明的，发展就快点。 三是地方支持力度，地方支持大的发展快些。 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:0:3","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"4、注意 可能有些院校的分校会非常偏僻，整体条件非常差。请对自己选的学校自己查询具体情况。 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:0:4","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"分校整理 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:1:0","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"985 1.哈尔滨工业大学(威海) 2.哈尔滨工业大学(深圳) 3.山东大学威海分校 4.大连理工大学(盘锦校区) 5.东北大学秦皇岛分校 6.电子科技大学(沙河校区) 7.中国人民大学(苏州校区) //貌似都是中外合办专业 8.厦门大学(马来西亚分校) //要去马来西亚 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:1:1","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"211 1.北京邮电大学(宏福校区) 2.合肥工业大学(宣城校区) 3.北京交通大学(威海校区) //貌似都是中外合办专业 4.河海大学(常州校区) 5.西南交通大学(峨眉校区) ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:1:2","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"几所特色211院校的京外校区 1.华北电力大学(保定) 2.中国石油大学(华东) 3.中国地质大学(武汉) 4.中国矿业大学(徐州) ps：这些院校的外地校区不一定比北京的分数低 ","date":"2020-06-11","objectID":"/2020/06/yidi985211/:1:3","tags":["高考志愿"],"title":"【高考志愿辅导】211、985院校异地校区（分校）解析","uri":"/2020/06/yidi985211/"},{"categories":["浙江高考志愿辅导"],"content":"有些学生想通过先进学校后转专业来到自己的理想专业，有一些注意事项","date":"2020-06-11","objectID":"/2020/06/zhuanzy/","tags":["高考志愿"],"title":"【高考志愿辅导】想通过先进学校后转专业的注意事项","uri":"/2020/06/zhuanzy/"},{"categories":["浙江高考志愿辅导"],"content":"写在前面 本专栏的高考志愿填报的文章主要是针对浙江省志愿填报的，其他省份可以参考，也有可能有部分内容不一致。 ","date":"2020-06-11","objectID":"/2020/06/zhuanzy/:0:1","tags":["高考志愿"],"title":"【高考志愿辅导】想通过先进学校后转专业的注意事项","uri":"/2020/06/zhuanzy/"},{"categories":["浙江高考志愿辅导"],"content":"注意事项 1.首先要明确的是，不同学校的转专业政策是不同的，有的学校转专业没有限制可以随意转,有的学校需要一定的要求。 2.具体学校的转专业政策可以通过咨询在校师生，查询学校教务处网站，查询学校招生网，拨打学校招生办电话等途径去了解。 3.转专业一般是在大一升大二的阶段完成(当然也不绝对，看具体学校)，所以意味着你要学完大一的课程，虽然大一的课程大部分为基础公共课，不排除个别专业基础课的开设。也就是说，你大一学的东西在你转专业后可能就完全没有用了，甚至你还要在大二时期补.上你转进专业大一你没学过他们学过的课程。你需要付出加倍的时间和精力。 4.对于一些大一就大量接触专业课的专业，可能需要降级转专业。比如临床医学，语言类等等。举个例子，你想转到德语专业，人家大一天天_上德语，德语听力，德语口语，德语写作什么的，你都没学过，除非你德语基础很强能通过老师的考核，说不定可以平级转入。大学降级转专业是很需要勇气的。 5.对于院校转专业要求比较高的，比如只能专业前几名的转。不建议去冒这个风险。大学诱惑很多，谁也不知道你大一能不能坚持下来学习，是否一定能达到那个成绩。现在的想法是美好的，到时候却一点办法没有，然后混个四年。(这样的实例是不少的)特别是某些专业在某个学校特别火，他招的人又少，就需要更高的成绩等等。 6.不同学校有不同的政策，可能转专业要换寝室，你可能又要重新开始适应室友，新同学，重新适应新的生活。 7.如果明确转专业，大一如果要考四六级的话,要认真准备四六级考试，积极参加学科竞赛。多加了解目标专业的情况，会给你增加优势。 8.整体而言，转专业还是要结合学校的转专业政策，转专业难度，自己承担学习任务的能力以及转出转入专业的特征进行综合分析和考量。 9.浙江有些高校在转专业的时候会要求选课。举个栗子：比如我政史地进了历史系，大二想进物理系会因为没有选考物理被直接拒绝。如温大，浙师大都是有卡这个的。 ps：关于转专业我还是想特别强调一点，浙江省的学校选课或成很大问题。哪怕前几年转专业不要求选课，今年就不一样了。毕竟今年招生都要求单限物理的很多，你们这届进去，明年可能就改政策了 ","date":"2020-06-11","objectID":"/2020/06/zhuanzy/:0:2","tags":["高考志愿"],"title":"【高考志愿辅导】想通过先进学校后转专业的注意事项","uri":"/2020/06/zhuanzy/"}]